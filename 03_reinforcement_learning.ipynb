{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Reinforcement learning\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Modell basiert"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Online vs. Batch"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Modell frei"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Passives lernen vs. aktives lernen\n",
    "Beides Arten von Reinforcement learning\n",
    "\n",
    "Passiv:\n",
    "* Agent hat die optimale Policy\n",
    "* Agent weiß, was er machen muss\n",
    "* Ziel: feste Abfolge von Aktionen ausführen, wie gut ist die optimale Policy?\n",
    "* ADP, TD\n",
    "\n",
    "Aktiv:\n",
    "* Agent kennt die optimale Policy nicht\n",
    "* Agent muss entscheiden, was er tun muss\n",
    "* Ziel: handeln und optimale Policy lernen\n",
    "* ADP mit exploration function, Q-learning\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Monte-Carlo-Methode\n",
    "\n",
    "* teilweise zufällig (randomisierter Algorithmus)\n",
    "* darf ein falsches Ergebnis liefern\n",
    "* häufig effizienter als deterministische Algorithmen"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Adaptive dynamic programming (ADP) \n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Temporal-difference learning (TD-Learning)\n",
    "\n",
    "* Agent passt Policy nach jeder Aktion auf Basis einer geschätzten erwarteten Belohnung an (nicht erst, wenn er die Belohnung erhält)\n",
    "* Funktion $V$ bewertet Zustand $s_t$\n",
    "* Mit jeder Iteration wird $V$ genauer, d.h. $V$ nähert sich $V^{\\pi}$ an\n",
    "* Lernrate: Wie gut passen neue Werte die Bewertungsfunktion $V$ an?\n",
    "\n",
    "$V(s_t)\\leftarrow(1-\\alpha)V(s_t)+\\alpha (r_{t+1}  \\gamma V(s_{t+1}))$ mit Lernrate $\\alpha$ und Discount-Faktor $\\gamma$"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Q-learning\n",
    "\n",
    "* Variante von TD-learning"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}