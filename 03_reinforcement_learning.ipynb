{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%HTML\n",
    "<style>\n",
    ".container { width:100% }\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement learning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO: Was ist Reinforcement Learning, Was sind die Vorteile, Allgemeines, etc.**\n",
    "\n",
    "Beim Reinforcement Learning  geht es darum, dass sich die KI ihr Modell innerhalb eines Lernprozesses selber entwickelt. Während des Lernprozesses wird mit Hilfe der `reward_function()` eine Strategie entwickelt. Diese Strategie kann mit verschiedenen Verfahren erstellt werden.\n",
    "\n",
    "Mit den bis jetzt definierten States, Aktionen, Rewards und der Umgebung können nun die Strategien für den Lernprozess angewendet werden."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Eine Methode des Maschinellen Lernens, wobei der Agent die Strategie zum Erreichen eines Zieles selber erlernen muss.\n",
    "- Versucht die Belohnungne die er durch die Belohnungsfunktion erhällt zu maximieren\n",
    "- Praktisch, wenn es keine Daten gibt, aus denen ein Modell erstellt werden kann\n",
    "- Lernt durch zufällige Trial-and-Error-Abläufe der Aktionen\n",
    "- Rein theoretisch kann eine KI nicht besser werden, als die Daten die sie bekommt, beim reinforcement lerning ist das anders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementierung 3\n",
    "Um die verschiedenen Algorithmen des Reinforcement Learnings wieder auf unser Beispiel anwenden zu können, wird zunächst eine allgemeine Funktion definiert."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`transport_goods()` führt so oft eine Aktion, die von der gewünschten `action_method()` ausgewählt wird, aus, bis die Ware an ihrem Zielort eingetroffen ist. Nach dem Ausführen wird die Darstellung falls gewünscht aktualisiert. Die Konstante `MAX_VISITS_STATE` gibt an, wie oft ein Zustand besucht werden kann, bevor ausgehend von diesem Zustand statt der eigentlich gewünschten Aktion eine zufällige Aktion gestartet wird. Diese Randomisierung ist notwendig, um zu verhindern, dass sich Aktions-Schleifen bilden, in denen der Agent festhängt. Im idealen Fall besucht der Agenten einen State nur ein mal (Position zweimal, einmal mit Paktet, einmal ohne). Der Wert wird trotzdem höher angesetzt, um keine zu starke Einschränkung zu bieten.\n",
    "\n",
    "Bei der Funktion handelt es sich auch um eine Funktion der Klasse `Transport`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_VISITS_STATE = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transport_goods(self):\n",
    "    position_truck, position_goods, position_goal = self.state\n",
    "    while not self.done:    \n",
    "        self.steps += 1\n",
    "        state_num = state_to_state_num(self.state)\n",
    "        action = self.action_method(state_num)\n",
    "        if action not in possible_actions[position_truck]:\n",
    "            action = random.sample(possible_actions[position_truck], 1)[0]\n",
    "        \n",
    "        value = self.reward_function(action)\n",
    "        new_state = self.transition_function(action)\n",
    "        if self.visited_states.count(new_state) > MAX_VISITS_STATE:\n",
    "            action = random.sample(possible_actions[position_truck], 1)[0]\n",
    "            value  = self.reward_function(action)\n",
    "            new_state = self.transition_function(action)\n",
    "        \n",
    "        self.current_value += value\n",
    "        self.state = new_state\n",
    "        self.visited_states.append(self.state)\n",
    "        position_truck, position_goods, position_goal = self.state\n",
    "        \n",
    "        if position_goods == position_goal: self.done = True\n",
    "        if self. visualize:\n",
    "            action_label.value = 'Action: ' + actions_description[action]\n",
    "            update_canvas(self)\n",
    "        if self.stepwise: time.sleep(0.1)\n",
    "Transport.transport_goods = transport_goods\n",
    "del transport_goods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Im folgenden werden alle möglichen Zustände in einer Liste gespeichert, sodass sie nummerierbar und damit eindeutig identifizierbar sind. Die nachfolgenden Funktionen `state_to_state_num()` und `state_num_to_state()` dienen dazu, zwischen den beiden Darstellungsweisen zu wechseln."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states = []\n",
    "for row in range (0, 6):\n",
    "    for col in range (0, 6):\n",
    "        for position_goods in range (0, 5):\n",
    "            for position_goal in range (0, 4):\n",
    "                states.append(((row, col), position_goods, position_goal))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def state_to_state_num(state):\n",
    "    return states.index(state)\n",
    "\n",
    "def state_num_to_state(num):\n",
    "    return states[num]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um einen Vergleich zu haben, wie der Transport ohne Reinforcement Learning aussehen würde, wird die folgende Methode genutzt. Die Funktion `random_action()` gibt eine zufällige Aktion zurück. Sie nutzt also keinen Lerneffekt oder Trainingsdaten. Im weiteren Verlauf der Arbeit werden verschiedene Algorithmen zur Auswahl der Aktionen implementiert, die genau dies tun. In der abschließenden Evaluation wird dann zu sehen sein, wie sich die Nutzung dieser Daten auf die Effizienz des Transports auswirkt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_action(state_num):\n",
    "    return random.randrange(0, 6, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modell basiert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Was bedeutet Modell basiert?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modell frei\n",
    "* Was bedeutet Modell frei?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Passives lernen vs. aktives lernen\n",
    "Reinforcement Learning Methoden lassen sich in zwei weitere Kategorien aufteilen. Beim passiven Lernen steht eine optimale Policy für den Agenten bereits und er muss bewerten, wie gut die ausführbaren Aktionen sind. Muss der Agent noch lernen, was er tun soll, so spricht man vom aktiven Lernen.\n",
    "\n",
    "Zunächst werfen wir ein Blick auf passives Lernen. Kennt der Agent die optimale Strategie bereits, um ein Problem zu lösen bedeutet das, dass die Policy $\\pi$ bekannt ist. Somit führt er für einen Zustand $s$ immer die Aktion $\\pi(s)$ aus. Es gilt herauszufinden, wie sinnvoll und nützlich dies ist. Die Funktion die dies ermittelt nennt man Utility-Funktion $U^{\\pi}(s)$. Diese Funktion $U$ gilt es zu finden. Algorithmen, die dies ermöglichen sind zum Beispiel Adaptive dynamic programming (ADP) und Temporal difference learning (TD-learning).\n",
    "\n",
    "Die zweite Kategorie ist die Gruppe der Probleme, bei denen der Agent zunächst eine Strategie für die gewünschte Aktion ermitteln muss. Er kennt die optimale Policy $\\pi$ nicht. Er muss also in jedem Zustand selbst entscheiden, was die beste nächste Aktion ist. So entsteht ein Modell, dass er immer weiter verbessert, um in Zukunft größere Belohnungen zu erhalten.\n",
    "Dabei kann es passieren, dass er annimmt einen optimalen Weg gefunden zu haben und diesen immer wieder geht. Andere vielleicht genauso gute oder sogar bessere Zustände und Optionen werden nicht mehr betrachtet. Dieses Problem bezeichnet man auch als \"exploitation vs. exploration\". Bei der exploitation (dt. \"Ausbeutung\") macht der Agent das zuvor beschribene: er nutzt Wege, von denen er gelernt hat, dass sie gut sind und optimiert sie gebenenfalls weiter. Exploration hingegen bezeichnet die Erkundung von neuen Zuständen. Versucht der Agent nur neue Zustände zu besuchen und neue Wege zu analysieren, ist dies auch nicht zielführend. Denn was nützt es, wenn man einen guten Weg kennt, aber ihn die nutzt? Der Agent muss also einen Kompromiss zwischen exploitation und exploration eingehen, um sowohl die kurzfristige als auch langfristige Belohnung zu maximieren. Später wird dafür der Wert $\\epsilon$ eingeführt werden. In $\\epsilon$-Prozent der Fälle geht der Agent dann neue Wege. In allen anderen Fällen versucht er die bekannten Wege zu gehen und zu lernen. Algorithmen mit denen dies möglich ist sind zum Beispiel ADP mit einer zusätzlichen exploration-Funktion oder der gleich vorgestellte Q-learning-Algorithmus.\n",
    "(vgl. Russell und Norvig 2010)\n",
    "\n",
    "Bei unserem Speditions-Beispiel handelt es sich gemäß der zuvor erläuterten Theorie um aktives lernen. Denn unser Agent, der LKW, hat von einem Feld ausgehend häufig viele Optionen in welche Richtung er fährt oder ob er Ware abliefert oder einsammelt. Er weiß, welche Aktionen in welchem Zustand möglich sind, kennt aber ohne Lernvorgänge nicht die Aktion, die am schnellsten zum Ziel führt.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Adaptive dynamic programming (ADP) )\n",
    "**TODO: Wollen wir das noch beschreiben: Wenn ja, beschreiben, sonst löschen. Easy :)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monte-Carlo-Methode\n",
    "\n",
    "Die Monte-Carlo-Methode ist eine Methode zur Ermittlung der zu erwartenden Belohnung beim Reinforcement Learning. Das besondere bei Monte-Carlo ist, dass es sich um einen randomisierten Algorithmus handelt. Als Vorraussetzung werden Erfahrungswerte, also Trainingsdaten bestehend aus einer Folge von Zuständen, Aktionen und Belohnungen benötigt. Die Idee ist, aus diesen Daten den durchschnittlich erreichten Wert nach besuchen eines Zustands $s$ zu ermitteln. Dazu gibt es zwei Ansätze: \"Every-Visit\" und \"First-Visit\".\n",
    "\n",
    "Bei \"First-Visit\" werden alle Belohnungen, die zwischen dem ersten Besuch des betrachteten Zustands $s$ und dem Endzustand auftreten, aufsummiert. Die so berechnetetn Werte für alle Episoden werden aufaddiert und durch die Anzahl der betrachteten Episoden geteilt.\n",
    "\n",
    "**TODO: Beispielgraph einfügen**\n",
    "\n",
    "Bei \"Every-Visit\" wird ein neuer Summations-Term für jedes Auftreten des Zustands $s$ erstellt. Das bedeutet man summiert zunächst alle Belohnung zwischen dem ersten Auftreten und dem Endzustand, addiert dazu die Summe der Belohnungen zwischen dem zweiten Auftreten und dem Endzustand und so weiter bis zur Addition der Summe der Belohnungen zwischen dem letzten Auftreten und dem Endzustand. Dies wird für alle Episoden wiederholt, aufaddiert und durch die Anzahl der Summations-Terme, also die Anzahl des Auftretens des Zustands $s$, dividiert.\n",
    "\n",
    "**TODO: Beispielgraph einfügen**\n",
    "\n",
    "(vgl. Gupta 2020, Sutton 2015)\n",
    "\n",
    "In der Anwendung von Monte-Carlo wird eine Episode von einem zufälligen Zustand bis zum Endzustand mit zufälligen Aktionen durchgespielt und dabei Zustände, Aktionen und Belohnungen aufgezeichnet. Anschließend werden die Werte der Zustände nach einer der oben erläuterten Methode berechnet. Anschließend wird ein neuer zufälliger Zustand bestimmt und unter Ausnutzung der zuvor berechneten Werte bis zum Ende geführt. Die Werte werden nach dem Durchgang neu berechnet und gemittelt. Dies wird viele Male wiederholt. So ergibt sich die gewünschte Werte-Matrix Q.\n",
    "\n",
    "Bei diesem Algorithmus ist nicht sichergestellt, dass jeder Zustand nach dem Training besucht wurde. Ein weiterer Nachteil ist, dass das Problem terminieren muss, bevor die Werte berechnet werden können. Es muss sich also zwingend um terminirende Probleme handeln."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementierung 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Implementierung\n",
    "def monte_carlo_action(state_num):\n",
    "    return "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Temporal-difference learning (TD-Learning)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ähnlich wie bei Monte Carlo, wird Temporal-Difference-Learning zur Berechnung der Q-Matrix verwendet. Bei jedem Durchlauf entscheidet sich der Agent für die Aktion, dessen Folgestate die höchsten Belohnung bereitstellt. Im Anschluss wird der Wert innerhalb der Q-Matrix mit Hilfe der Value-Funktion nach folgender Formel angepasst:\n",
    "    \n",
    "$V(s_t)\\leftarrow(1-\\alpha)V(s_t)+\\alpha(r_{t+1} + \\gamma V(s_{t+1}))$\n",
    "\n",
    "wobei gilt:\n",
    "* $V(s_t)$ ist der Wert des aktuellen States\n",
    "* $r_t+1$ ist der Reward, der im nächstem Schritt erhalten wird\n",
    "* $\\gamma$ ist die Gewichtung, wie hoch der Einfluss des Wertes in den Gesamtwert eingeht (engl. Discount-Factor) \n",
    "* $\\alpha$ ist die Lernrate\n",
    "\n",
    "Der Teil $r_{t+1} + \\gamma V(s_{t+1})$ wir auch als TargetError bezichnet und gibt **(TODO)** an.\n",
    "\n",
    "Nach jeder Aktion des Agenten wird die Q-Matrix optimiert und $V$ nähert sich $V^{\\pi}$ an. Im Gegensatz zu Monte Carlo wird die Matrix also nicht erst angepasst, wenn ein Reward erhalten wurde. Zudem werden die Rewards, mit einem Gewicht $\\gamma$ versehen. Dadurch wird aktuelleren Werten eine höhere Wertigkeit im Endergebnis gegeben. Der Vorteil dabei ist, dass die Varianz minimiert wird. Ausreißer haben also kleinere Auswirkungen auf das Endergebnis.\n",
    "\n",
    "**TODO: Beispielgraph einfügen**\n",
    "\n",
    "(vgl. Kumar 2019)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementierung 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Implementierung\n",
    "def td_learning_action(state_num):\n",
    "    return "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q-learning\n",
    "\n",
    "* Variante von TD-learning\n",
    "\n",
    "- Modellfreier R.L. Algorythmus um der Wert einer Aktion zu berechnen\n",
    "- Benötigt keine Anpassung des Enviroments, nut Rewards und stochastische Transitionen\n",
    "- Für einen Endlichen Markov Decision Prozess, findet Q-learning die Optimale Policy(Regel), um eine optimale Aktionsauswahl zu treffen, auf Basis von zufälligen Aktionen, falls man unendlich Zeit zur Verfügung hat.\n",
    "- In eine Matrix len(States)xlen(Aktionen) werden die Rewards für eine Aktion im State eingetragen.\n",
    "- Initial ist die Matrix mit 0 gefüllt\n",
    "- Die Q-Funktion: Q: S x A -> R mappt eine Aktion und ein State auf eine Rationale zahl, die in der Matrix eingetragen wird\n",
    "- Neue Werte werden mit einer höheren Gewichtung in der Matirx einberechnet (gamma, discount factor)\n",
    "- in der Tabelle sind die Werte der letzen States aufsummiert eingetragen, also die Rewards der ganzen Weges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementierung 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zunächst wird eine Tabelle mit einer Zeile für jeden Zustand und einer Spalte für jede Aktion angelegt. In jede Zelle wird zu Beginn der Wert Null geschrieben."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "q_table = np.zeros([len(states), len(actions)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die `q_function()` nimmt einen Zustand und eine Aktion sowie die Lernrate $\\alpha$ und den Discount-Faktor $\\gamma$. Darauf basierend berechnet sie mit Hilfe der `q_table` den neuen Wert für den Zustand und die Aktion und trägt diesen an der entsprechenden Stelle in der Tabelle ein. Zurückgegeben wird außerdem der nächste Zustand. Die Formel für die Berechnung des neuen Wertes lautet folgendermaßen:\n",
    "\n",
    "\\\\[Q_{\\texttt{new}}(s_t, a_t) \\leftarrow Q_{\\texttt{old}}(s_t, a_t) + \\alpha \\cdot (r_t + \\gamma \\cdot max_q(s{t+1}, a) - Q_{\\texttt{old}}(s_t, a_t))\\\\]\n",
    "\n",
    "mit:\n",
    "* $s_t$, $s_{t+1} \\in$ len(states)\n",
    "* a $\\in$ actions\n",
    "* $Q_{\\texttt{old}}(s_t, a_t)$ = Tabellenwert in Zeile $s_t$ und Spalte $a_t$\n",
    "* $\\alpha$ = Lernrate\n",
    "* $\\gamma$ = Discount-Faktor\n",
    "* $r_t$ = Reward\n",
    "* max_q = Schätzung des optimalen Zukunftswertes\n",
    "\n",
    "Der Wert in der Klammer wird dabei auch als \"temporal difference\" bezeichnet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_function(state, action, alpha, gamma):\n",
    "    transport  = Transport(state, stepwise = False, visualize = False)\n",
    "    \n",
    "    reward     = transport.reward_function(action)\n",
    "    next_state = transport.transition_function(action)\n",
    "    \n",
    "    state_num      = state_to_state_num(state)\n",
    "    next_state_num = state_to_state_num(next_state)\n",
    "    \n",
    "    old_value  = q_table[state_num][action]\n",
    "    max_q      = np.max(q_table[next_state_num])\n",
    "    new_value  = old_value + alpha * (reward + gamma * max_q - old_value)\n",
    "    \n",
    "    q_table[state_num, action] = new_value\n",
    "    return next_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Funktion `q_learning()` führt für eine zu übergebende Anzahl an Startzuständen (`episodes`) den Transport der Waren zum Ziel durch. Der Startzustand ist dabei ein zufälliger. Dies dient dem Training und somit der Verbesserung der `q_table`. Denn in jedem Schritt wird `q_function()` aufgerufen.  In rund 10% (= `epsilon`) der Schritte ist auch die Aktion zufällig ausgewählt. In den anderen Fällen wird die am besten bewertete Aktion für den aktuellen Zustand genutzt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_learning(episodes):\n",
    "    alpha   = 0.1\n",
    "    gamma   = 0.6\n",
    "    epsilon = 0.1\n",
    "    \n",
    "    for episode in range(episodes):\n",
    "        state = states[random.randrange(0, len(states), 1)]\n",
    "        done  = False\n",
    "        \n",
    "        while not done:\n",
    "            state_num = state_to_state_num(state)\n",
    "            if random.uniform(0, 1) < epsilon:\n",
    "                action = random.sample(possible_actions[state[0]], 1)[0] # Aktionsbereich erkunden\n",
    "            else:\n",
    "                action = np.argmax(q_table[state_num]) # Gelernte Werte ausnutzen\n",
    "                \n",
    "            state = q_function(state, action, alpha, gamma)\n",
    "            _, position_goods, position_goal = state\n",
    "            if position_goods == position_goal: done = True\n",
    "        print(round(episode + 1/episodes*100, 1), '%', end='\\r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zum Trainieren muss die Funktion `q_learning()` mit einem relativ hohen Wert aufgerufen werden. Damit dies nicht nach jedem Neustart des Kernels notwendig ist, kann die Tabelle abgespeichert und beim nächsten mal einfach gelade werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#q_learning(10000) # Trainieren\n",
    "#np.savetxt('q_table.txt', q_table) # Speichern der Tabelle\n",
    "q_table = np.loadtxt('q_table.txt') # Laden der Tabelle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die folgende Methode kann dann nach dem Trainieren in der tatsächlichen Anwendung genutzt werden, üm für jeden Zustand die beste Aktion zu ermitteln. Dazu muss sie der Klasse `Transport` bei der Initialisierung übergeben werden und wird später dann von `transport_goods()` aufgerufen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_learning_action(state_num):\n",
    "    return np.argmax(q_table[state_num]) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
