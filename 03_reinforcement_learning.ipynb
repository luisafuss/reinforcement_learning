{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement learning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modell basiert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Was bedeutet Modell basiert?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modell frei\n",
    "* Was bedeutet Modell frei?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Passives lernen vs. aktives lernen\n",
    "Beides Arten von Reinforcement learning\n",
    "\n",
    "Passiv:\n",
    "* Agent hat die optimale Policy\n",
    "* Agent weiß, was er machen muss\n",
    "* Ziel: feste Abfolge von Aktionen ausführen, wie gut ist die optimale Policy?\n",
    "* ADP, TD\n",
    "\n",
    "Aktiv:\n",
    "* Agent kennt die optimale Policy nicht\n",
    "* Agent muss entscheiden, was er tun muss\n",
    "* Ziel: handeln und optimale Policy lernen\n",
    "* ADP mit exploration function, Q-learning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Adaptive dynamic programming (ADP) )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monte-Carlo-Methode\n",
    "\n",
    "* teilweise zufällig (randomisierter Algorithmus)\n",
    "* darf ein falsches Ergebnis liefern\n",
    "* häufig effizienter als deterministische Algorithmen\n",
    "- Random alle aktionen ausführen und den Avg der Rewards berechnen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Temporal-difference learning (TD-Learning)\n",
    "\n",
    "* Agent passt Policy nach jeder Aktion auf Basis einer geschätzten erwarteten Belohnung an (nicht erst, wenn er die Belohnung erhält)\n",
    "* Funktion $V$ bewertet Zustand $s_t$\n",
    "* Mit jeder Iteration wird $V$ genauer, d.h. $V$ nähert sich $V^{\\pi}$ an\n",
    "* Lernrate: Wie gut passen neue Werte die Bewertungsfunktion $V$ an?\n",
    "\n",
    "$V(s_t)\\leftarrow(1-\\alpha)V(s_t)+\\alpha (r_{t+1}  \\gamma V(s_{t+1}))$ mit Lernrate $\\alpha$ und Discount-Faktor $\\gamma$\n",
    "\n",
    "- mit Gewichtung der neuen Aktionen\n",
    "- Vorteil: neue Aktionen haben eine höhere Auswirkungen auf das Model, minimiert die Variazn im vergleich zu monte carlo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ähnlich wie bei Monte Carlo, wird Temporal difference learning zur Berechnung der Q-Matrix verwendet. Bei jedem Durchlauf entscheidet sich der Agent für die Aktion, die zu dem State mit dem höchsten Reward führt. Im Anschluss wird der Wert mit Hilfe der Value-Funktion nach folgender Formel angepasst:\n",
    "    \n",
    "$V(s_t)\\leftarrow(1-\\alpha)V(s_t)+\\alpha(r_{t+1} \\gamma V(s_{t+1}))$\n",
    "\n",
    "wobei gilt:\n",
    "* $V(s_t)$ ist der Wert des aktuellen States\n",
    "* $r_t+1$ ist der Reward, der im nächstem Schritt erhalten wird\n",
    "* $\\gamma$ ist die Gewichtung, wie hoch der Einfluss des Wertes in den Gesamtwert eingeht\n",
    "* $\\alpha$ ist die Lernrate\n",
    "\n",
    "Die Matrix optimiert sich direkt nach jedem Trainings-Durchlauf. Im Gegensatz zu Monte Carlo werden die Rewards, mit einem Gewicht $\\gamma$ versehen. Dadurch wird aktuelleren Werten eine höhere Wertigkeit im Endergebnis gegeben. Der Vorteil dabei ist, dass die Varianz minimiert wird. Ausreißer haben also kleinere Auswirkungen auf das Endergebnis.\n",
    "\n",
    "**Beispielgraph einfügen**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q-learning\n",
    "\n",
    "* Variante von TD-learning\n",
    "\n",
    "- Modellfreier R.L. Algorythmus um der Wert einer Aktion zu berechnen\n",
    "- Benötigt keine Anpassung des Enviroments, nut Rewards und stochastische Transitionen\n",
    "- Für einen Endlichen Markov Decision Prozess, findet Q-learning die Optimale Policy(Regel), um eine optimale Aktionsauswahl zu treffen, auf Basis von zufälligen Aktionen, falls man unendlich Zeit zur Verfügung hat.\n",
    "- In eine Matrix len(States)xlen(Aktionen) werden die Rewards für eine Aktion im State eingetragen.\n",
    "- Initial ist die Matrix mit 0 gefüllt\n",
    "- Die Q-Funktion: Q: S x A -> R mappt eine Aktion und ein State auf eine Rationale zahl, die in der Matrix eingetragen wird\n",
    "- Neue Werte werden mit einer höheren Gewichtung in der Matirx einberechnet (gamma, discount factor)\n",
    "- in der Tabelle sind die Werte der letzen States aufsummiert eingetragen, also die Rewards der ganzen Weges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
