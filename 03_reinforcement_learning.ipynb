{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement learning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO: Was ist Reinforcement Learning, Was sind die Vorteile, Allgemeines, etc.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementierung 3\n",
    "Um die verschiedenen Algorithmen des Reinforcement Learnings wieder auf unser Beispiel anwenden zu können, wird zunächst eine allgemeine Funktion definiert."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`transport_goods()` führt so oft eine Aktion, die von der gewünschten `action_method()` ausgewählt wird, aus, bis die Ware an ihrem Zielort eingetroffen ist. Nach dem Ausführen wird die Darstellung falls gewünscht aktualisiert. Die Konstante `MAX_VISITS_STATE` gibt an, wie oft ein Zustand besucht werden kann, bevor ausgehend von diesem Zustand statt der eigentlich gewünschten Aktion eine zufällige Aktion gestartet wird. Diese Randomisierung ist notwendig, um zu verhindern, dass sich Aktions-Schleifen bilden, in denen der Agent festhängt. Im idealen Fall besucht der Agenten einen State nur ein mal (Position zweimal, einmal mit Paktet, einmal ohne). Der Wert wird trotzdem höher angesetzt, um keine zu starke Einschränkung zu bieten.\n",
    "\n",
    "Bei der Funktion handelt es sich auch um eine Funktion der Klasse `Transport`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_VISITS_STATE = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transport_goods(self):\n",
    "    position_truck, position_goods, position_goal = self.state\n",
    "    while not self.done:    \n",
    "        self.steps += 1\n",
    "        state_num = state_to_state_num(self.state)\n",
    "        action = self.action_method(state_num)\n",
    "        if action not in possible_actions[position_truck]:\n",
    "            action = random.sample(possible_actions[position_truck], 1)[0]\n",
    "        \n",
    "        value = self.reward_function(action)\n",
    "        new_state = self.transition_function(action)\n",
    "        if self.visited_states.count(new_state) > MAX_VISITS_STATE:\n",
    "            action = random.sample(possible_actions[position_truck], 1)[0]\n",
    "            value  = self.reward_function(action)\n",
    "            new_state = self.transition_function(action)\n",
    "        \n",
    "        self.current_value += value\n",
    "        self.state = new_state\n",
    "        self.visited_states.append(self.state)\n",
    "        position_truck, position_goods, position_goal = self.state\n",
    "        \n",
    "        if position_goods == position_goal: self.done = True\n",
    "        if self. visualize:\n",
    "            action_label.value = 'Action: ' + actions_description[action]\n",
    "            update_canvas(self)\n",
    "        if self.stepwise: time.sleep(0.1)\n",
    "Transport.transport_goods = transport_goods\n",
    "del transport_goods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Im folgenden werden alle möglichen Zustände in einer Liste gespeichert, sodass sie nummerierbar und damit eindeutig identifizierbar sind. Die nachfolgenden Funktionen `state_to_state_num()` und `state_num_to_state()` dienen dazu, zwischen den beiden Darstellungsweisen zu wechseln."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states = []\n",
    "for row in range (0, 6):\n",
    "    for col in range (0, 6):\n",
    "        for position_goods in range (0, 5):\n",
    "            for position_goal in range (0, 4):\n",
    "                states.append(((row, col), position_goods, position_goal))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def state_to_state_num(state):\n",
    "    return states.index(state)\n",
    "\n",
    "def state_num_to_state(num):\n",
    "    return states[num]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um einen Vergleich zu haben, wie der Transport ohne Reinforcement Learning aussehen würde, wird die folgende Methode genutzt. Die Funktion `random_action()` gibt eine zufällige Aktion zurück. Sie nutzt also keinen Lerneffekt oder Trainingsdaten. Im weiteren Verlauf der Arbeit werden verschiedene Algorithmen zur Auswahl der Aktionen implementiert, die genau dies tun. In der abschließenden Evaluation wird dann zu sehen sein, wie sich die Nutzung dieser Daten auf die Effizienz des Transports auswirkt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_action(state_num):\n",
    "    return random.randrange(0, 6, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modell basiert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Was bedeutet Modell basiert?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modell frei\n",
    "* Was bedeutet Modell frei?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Passives lernen vs. aktives lernen\n",
    "Beides Arten von Reinforcement learning\n",
    "\n",
    "Passiv:\n",
    "* Agent hat die optimale Policy\n",
    "* Agent weiß, was er machen muss\n",
    "* Ziel: feste Abfolge von Aktionen ausführen, wie gut ist die optimale Policy?\n",
    "* ADP, TD\n",
    "\n",
    "Aktiv:\n",
    "* Agent kennt die optimale Policy nicht\n",
    "* Agent muss entscheiden, was er tun muss\n",
    "* Ziel: handeln und optimale Policy lernen\n",
    "* ADP mit exploration function, Q-learning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Adaptive dynamic programming (ADP) )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monte-Carlo-Methode\n",
    "\n",
    "Die Monte-Carlo-Methode ist eine Methode zur Ermittlung der zu erwartenden Belohnung beim Reinforcement Learning. Das besondere bei Monte-Carlo ist, dass es sich um einen randomisierten Algorithmus handelt. Als Vorraussetzung werden Erfahrungswerte, also Trainingsdaten bestehend aus einer Folge von Zuständen, Aktionen und Belohnungen benötigt. Die Idee ist, aus diesen Daten den durchschnittlich erreichten Wert nach besuchen eines Zustands $s$ zu ermitteln. Dazu gibt es zwei Ansätze: \"Every-Visit\" und \"First-Visit\".\n",
    "\n",
    "Bei \"First-Visit\" werden alle Belohnungen, die zwischen dem ersten Besuch des betrachteten Zustands $s$ und dem Endzustand auftreten, aufsummiert. Die so berechnetetn Werte für alle Episoden werden aufaddiert und durch die Anzahl der betrachteten Episoden geteilt.\n",
    "\n",
    "**TODO: Beispielgraph einfügen**\n",
    "\n",
    "Bei \"Every-Visit\" wird ein neuer Summations-Term für jedes Auftreten des Zustands $s$ erstellt. Das bedeutet man summiert zunächst alle Belohnung zwischen dem ersten Auftreten und dem Endzustand, addiert dazu die Summe der Belohnungen zwischen dem zweiten Auftreten und dem Endzustand und so weiter bis zur Addition der Summe der Belohnungen zwischen dem letzten Auftreten und dem Endzustand. Dies wird für alle Episoden wiederholt, aufaddiert und durch die Anzahl der Summations-Terme, also die Anzahl des Auftretens des Zustands $s$, dividiert.\n",
    "\n",
    "**TODO: Beispielgraph einfügen**\n",
    "\n",
    "(vgl. Gupta 2020, Sutton 2015)\n",
    "\n",
    "In der Anwendung von Monte-Carlo wird eine Episode von einem zufälligen Zustand bis zum Endzustand mit zufälligen Aktionen durchgespielt und dabei Zustände, Aktionen und Belohnungen aufgezeichnet. Anschließend werden die Werte der Zustände nach einer der oben erläuterten Methode berechnet. Anschließend wird ein neuer zufälliger Zustand bestimmt und unter Ausnutzung der zuvor berechneten Werte bis zum Ende geführt. Die Werte werden nach dem Durchgang neu berechnet und gemittelt. Dies wird viele Male wiederholt. So ergibt sich die gewünschte Werte-Matrix Q.\n",
    "\n",
    "Bei diesem Algorithmus ist nicht sichergestellt, dass jeder Zustand nach dem Training besucht wurde. Ein weiterer Nachteil ist, dass das Problem terminieren muss, bevor die Werte berechnet werden können. Es muss sich also zwingend um terminirende Probleme handeln."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementierung 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Implementierung\n",
    "def monte_carlo_action(state_num):\n",
    "    return "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Temporal-difference learning (TD-Learning)\n",
    "\n",
    "* Agent passt Policy nach jeder Aktion auf Basis einer geschätzten erwarteten Belohnung an (nicht erst, wenn er die Belohnung erhält)\n",
    "* Funktion $V$ bewertet Zustand $s_t$\n",
    "* Mit jeder Iteration wird $V$ genauer, d.h. $V$ nähert sich $V^{\\pi}$ an\n",
    "* Lernrate: Wie gut passen neue Werte die Bewertungsfunktion $V$ an?\n",
    "\n",
    "$V(s_t)\\leftarrow(1-\\alpha)V(s_t)+\\alpha (r_{t+1}  \\gamma V(s_{t+1}))$ mit Lernrate $\\alpha$ und Discount-Faktor $\\gamma$\n",
    "\n",
    "- mit Gewichtung der neuen Aktionen\n",
    "- Vorteil: neue Aktionen haben eine höhere Auswirkungen auf das Model, minimiert die Variazn im vergleich zu monte carlo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ähnlich wie bei Monte Carlo, wird Temporal difference learning zur Berechnung der Q-Matrix verwendet. Bei jedem Durchlauf entscheidet sich der Agent für die Aktion, die zu dem State mit dem höchsten Reward führt. Im Anschluss wird der Wert mit Hilfe der Value-Funktion nach folgender Formel angepasst:\n",
    "    \n",
    "$V(s_t)\\leftarrow(1-\\alpha)V(s_t)+\\alpha(r_{t+1} \\gamma V(s_{t+1}))$\n",
    "\n",
    "wobei gilt:\n",
    "* $V(s_t)$ ist der Wert des aktuellen States\n",
    "* $r_t+1$ ist der Reward, der im nächstem Schritt erhalten wird\n",
    "* $\\gamma$ ist die Gewichtung, wie hoch der Einfluss des Wertes in den Gesamtwert eingeht\n",
    "* $\\alpha$ ist die Lernrate\n",
    "\n",
    "Die Matrix optimiert sich direkt nach jedem Trainings-Durchlauf. Im Gegensatz zu Monte Carlo werden die Rewards, mit einem Gewicht $\\gamma$ versehen. Dadurch wird aktuelleren Werten eine höhere Wertigkeit im Endergebnis gegeben. Der Vorteil dabei ist, dass die Varianz minimiert wird. Ausreißer haben also kleinere Auswirkungen auf das Endergebnis.\n",
    "\n",
    "**TODO: Beispielgraph einfügen**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementierung 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Implementierung\n",
    "def td_learning_action(state_num):\n",
    "    return "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q-learning\n",
    "\n",
    "* Variante von TD-learning\n",
    "\n",
    "- Modellfreier R.L. Algorythmus um der Wert einer Aktion zu berechnen\n",
    "- Benötigt keine Anpassung des Enviroments, nut Rewards und stochastische Transitionen\n",
    "- Für einen Endlichen Markov Decision Prozess, findet Q-learning die Optimale Policy(Regel), um eine optimale Aktionsauswahl zu treffen, auf Basis von zufälligen Aktionen, falls man unendlich Zeit zur Verfügung hat.\n",
    "- In eine Matrix len(States)xlen(Aktionen) werden die Rewards für eine Aktion im State eingetragen.\n",
    "- Initial ist die Matrix mit 0 gefüllt\n",
    "- Die Q-Funktion: Q: S x A -> R mappt eine Aktion und ein State auf eine Rationale zahl, die in der Matrix eingetragen wird\n",
    "- Neue Werte werden mit einer höheren Gewichtung in der Matirx einberechnet (gamma, discount factor)\n",
    "- in der Tabelle sind die Werte der letzen States aufsummiert eingetragen, also die Rewards der ganzen Weges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementierung 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zunächst wird eine Tabelle mit einer Zeile für jeden Zustand und einer Spalte für jede Aktion angelegt. In jede Zelle wird zu Beginn der Wert Null geschrieben."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "q_table = np.zeros([len(states), len(actions)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die `q_function()` nimmt einen Zustand und eine Aktion sowie die Lernrate $\\alpha$ und den Discount-Faktor $\\gamma$. Darauf basierend berechnet sie mit Hilfe der `q_table` den neuen Wert für den Zustand und die Aktion und trägt diesen an der entsprechenden Stelle in der Tabelle ein. Zurückgegeben wird außerdem der nächste Zustand. Die Formel für die Berechnung des neuen Wertes lautet folgendermaßen:\n",
    "\n",
    "\\\\[Q_{\\texttt{new}}(s_t, a_t) \\leftarrow Q_{\\texttt{old}}(s_t, a_t) + \\alpha \\cdot (r_t + \\gamma \\cdot max_q(s{t+1}, a) - Q_{\\texttt{old}}(s_t, a_t))\\\\]\n",
    "\n",
    "mit:\n",
    "* $s_t$, $s_{t+1} \\in$ len(states)\n",
    "* a $\\in$ actions\n",
    "* $Q_{\\texttt{old}}(s_t, a_t)$ = Tabellenwert in Zeile $s_t$ und Spalte $a_t$\n",
    "* $\\alpha$ = Lernrate\n",
    "* $\\gamma$ = Discount-Faktor\n",
    "* $r_t$ = Reward\n",
    "* max_q = Schätzung des optimalen Zukunftswertes\n",
    "\n",
    "Der Wert in der Klammer wird dabei auch als \"temporal difference\" bezeichnet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_function(state, action, alpha, gamma):\n",
    "    transport  = Transport(state, stepwise = False, visualize = False)\n",
    "    \n",
    "    reward     = transport.reward_function(action)\n",
    "    next_state = transport.transition_function(action)\n",
    "    \n",
    "    state_num      = state_to_state_num(state)\n",
    "    next_state_num = state_to_state_num(next_state)\n",
    "    \n",
    "    old_value  = q_table[state_num][action]\n",
    "    max_q      = np.max(q_table[next_state_num])\n",
    "    new_value  = old_value + alpha * (reward + gamma * max_q - old_value)\n",
    "    \n",
    "    q_table[state_num, action] = new_value\n",
    "    return next_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Funktion `q_learning()` führt für eine zu übergebende Anzahl an Startzuständen (`episodes`) den Transport der Waren zum Ziel durch. Der Startzustand ist dabei ein zufälliger. Dies dient dem Training und somit der Verbesserung der `q_table`. Denn in jedem Schritt wird `q_function()` aufgerufen.  In rund 10% (= `epsilon`) der Schritte ist auch die Aktion zufällig ausgewählt. In den anderen Fällen wird die am besten bewertete Aktion für den aktuellen Zustand genutzt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_learning(episodes):\n",
    "    alpha   = 0.1\n",
    "    gamma   = 0.6\n",
    "    epsilon = 0.1\n",
    "    \n",
    "    for episode in range(episodes):\n",
    "        state = states[random.randrange(0, len(states), 1)]\n",
    "        done  = False\n",
    "        \n",
    "        while not done:\n",
    "            state_num = state_to_state_num(state)\n",
    "            if random.uniform(0, 1) < epsilon:\n",
    "                action = random.sample(possible_actions[state[0]], 1)[0] # Aktionsbereich erkunden\n",
    "            else:\n",
    "                action = np.argmax(q_table[state_num]) # Gelernte Werte ausnutzen\n",
    "                \n",
    "            state = q_function(state, action, alpha, gamma)\n",
    "            _, position_goods, position_goal = state\n",
    "            if position_goods == position_goal: done = True\n",
    "        print(round(episode + 1/episodes*100, 1), '%', end='\\r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zum Trainieren muss die Funktion `q_learning()` mit einem relativ hohen Wert aufgerufen werden. Damit dies nicht nach jedem Neustart des Kernels notwendig ist, kann die Tabelle abgespeichert und beim nächsten mal einfach gelade werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#q_learning(10000) # Trainieren\n",
    "#np.savetxt('q_table.txt', q_table) # Speichern der Tabelle\n",
    "q_table = np.loadtxt('q_table.txt') # Laden der Tabelle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die folgende Methode kann dann nach dem Trainieren in der tatsächlichen Anwendung genutzt werden, üm für jeden Zustand die beste Aktion zu ermitteln. Dazu muss sie der Klasse `Transport` bei der Initialisierung übergeben werden und wird später dann von `transport_goods()` aufgerufen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_learning_action(state_num):\n",
    "    return np.argmax(q_table[state_num]) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
