{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement learning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modell basiert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Online vs. Batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modell frei"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Passives lernen vs. aktives lernen\n",
    "Beides Arten von Reinforcement learning\n",
    "\n",
    "Passiv:\n",
    "* Agent hat die optimale Policy\n",
    "* Agent weiß, was er machen muss\n",
    "* Ziel: feste Abfolge von Aktionen ausführen, wie gut ist die optimale Policy?\n",
    "* ADP, TD\n",
    "\n",
    "Aktiv:\n",
    "* Agent kennt die optimale Policy nicht\n",
    "* Agent muss entscheiden, was er tun muss\n",
    "* Ziel: handeln und optimale Policy lernen\n",
    "* ADP mit exploration function, Q-learning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monte-Carlo-Methode\n",
    "\n",
    "* teilweise zufällig (randomisierter Algorithmus)\n",
    "* darf ein falsches Ergebnis liefern\n",
    "* häufig effizienter als deterministische Algorithmen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adaptive dynamic programming (ADP) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Temporal-difference learning (TD-Learning)\n",
    "\n",
    "* Agent passt Policy nach jeder Aktion auf Basis einer geschätzten erwarteten Belohnung an (nicht erst, wenn er die Belohnung erhält)\n",
    "* Funktion $V$ bewertet Zustand $s_t$\n",
    "* Mit jeder Iteration wird $V$ genauer, d.h. $V$ nähert sich $V^{\\pi}$ an\n",
    "* Lernrate: Wie gut passen neue Werte die Bewertungsfunktion $V$ an?\n",
    "\n",
    "$V(s_t)\\leftarrow(1-\\alpha)V(s_t)+\\alpha (r_{t+1}  \\gamma V(s_{t+1}))$ mit Lernrate $\\alpha$ und Discount-Faktor $\\gamma$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q-learning\n",
    "\n",
    "* Variante von TD-learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ähnlich wie bei Monte Carlo, wird Temporal different learning zur Berechnung der Q Matrix verwendet. Bei jedem Durchlauf entscheidet sich der Agent für die Aktion, die zu dem State mit dem höchsten Reward führt. Im Anschluss wird der Wert mit Hilfe der Value-Funktion nach folgender Formel angepasst:\n",
    "\n",
    "$V(s_t)=(1-alpha)V(s_t)+alpha(r_t+1+gammaV(s_t))$\n",
    "\n",
    "wobei gilt:\n",
    "* V(s_t) ist der Wert des aktuellen States\n",
    "* $r_t+1$ ist der Reward, der im nächstem Schritt erhalten wird\n",
    "* $gamma$ ist die Gewichtung, wie hoch der Einfluss des Wertes in den Gesamtwert eingeht\n",
    "* $alpha$ ist die Lernrate\n",
    "\n",
    "Die Matrix optimiert sich direkt nach jedem Training Durchlauf. Im Gegensatz zu Monte Carlo werden die Rewards, mit einem Gewicht $gamma§ versehen. Dadurch wird aktuelleren Werten eine höhere Wertigkeit im Endergebnis gegeben. Der Vorteil dabei ist, dass die Varianz minimiert wird. Ausreißer haben also kleinere Auswirkungen auf das Endergebnis.\n",
    "\n",
    "**Beispielgraph einfügen**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
