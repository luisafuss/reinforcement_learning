{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement learning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modell basiert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Was bedeutet Modell basiert?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modell frei\n",
    "* Was bedeutet Modell frei?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Passives lernen vs. aktives lernen\n",
    "Beides Arten von Reinforcement learning\n",
    "\n",
    "Passiv:\n",
    "* Agent hat die optimale Policy\n",
    "* Agent weiß, was er machen muss\n",
    "* Ziel: feste Abfolge von Aktionen ausführen, wie gut ist die optimale Policy?\n",
    "* ADP, TD\n",
    "\n",
    "Aktiv:\n",
    "* Agent kennt die optimale Policy nicht\n",
    "* Agent muss entscheiden, was er tun muss\n",
    "* Ziel: handeln und optimale Policy lernen\n",
    "* ADP mit exploration function, Q-learning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Adaptive dynamic programming (ADP) )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monte-Carlo-Methode\n",
    "\n",
    "Die Monte-Carlo-Methode ist eine Methode zur Ermittlung der zu erwartenden Belohnung beim Reinforcement Learning. Das besondere bei Monte-Carlo ist, dass es sich um einen randomisierten Algorithmus handelt. Als Vorraussetzung werden Erfahrungswerte, also Trainingsdaten bestehend aus einer Folge von Zuständen, Aktionen und Belohnungen benötigt. Die Idee ist, aus diesen Daten den durchschnittlich erreichten Wert nach besuchen eines Zustands $s$ zu ermitteln. Dazu gibt es zwei Ansätze: \"Every-Visit\" und \"First-Visit\".\n",
    "\n",
    "Bei \"First-Visit\" werden alle Belohnungen, die zwischen dem ersten Besuch des betrachteten Zustands $s$ und dem Endzustand auftreten, aufsummiert. Die so berechnetetn Werte für alle Episoden werden aufaddiert und durch die Anzahl der betrachteten Episoden geteilt.\n",
    "\n",
    "**TODO: Beispielgraph einfügen**\n",
    "\n",
    "Bei \"Every-Visit\" wird ein neuer Summations-Term für jedes Auftreten des Zustands $s$ erstellt. Das bedeutet man summiert zunächst alle Belohnung zwischen dem ersten Auftreten und dem Endzustand, addiert dazu die Summe der Belohnungen zwischen dem zweiten Auftreten und dem Endzustand und so weiter bis zur Addition der Summe der Belohnungen zwischen dem letzten Auftreten und dem Endzustand. Dies wird für alle Episoden wiederholt, aufaddiert und durch die Anzahl der Summations-Terme, also die Anzahl des Auftretens des Zustands $s$, dividiert.\n",
    "\n",
    "**TODO: Beispielgraph einfügen**\n",
    "\n",
    "(vgl. Gupta 2020, Sutton 2015)\n",
    "\n",
    "In der Anwendung von Monte-Carlo wird eine Episode von einem zufälligen Zustand bis zum Endzustand mit zufälligen Aktionen durchgespielt und dabei Zustände, Aktionen und Belohnungen aufgezeichnet. Anschließend werden die Werte der Zustände nach einer der oben erläuterten Methode berechnet. Anschließend wird ein neuer zufälliger Zustand bestimmt und unter Ausnutzung der zuvor berechneten Werte bis zum Ende geführt. Die Werte werden nach dem Durchgang neu berechnet und gemittelt. Dies wird viele Male wiederholt. So ergibt sich die gewünschte Werte-Matrix Q.\n",
    "\n",
    "Bei diesem Algorithmus ist nicht sichergestellt, dass jeder Zustand nach dem Training besucht wurde. Ein weiterer Nachteil ist, dass das Problem terminieren muss, bevor die Werte berechnet werden können. Es muss sich also zwingend um terminirende Probleme handeln."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Temporal-difference learning (TD-Learning)\n",
    "\n",
    "* Agent passt Policy nach jeder Aktion auf Basis einer geschätzten erwarteten Belohnung an (nicht erst, wenn er die Belohnung erhält)\n",
    "* Funktion $V$ bewertet Zustand $s_t$\n",
    "* Mit jeder Iteration wird $V$ genauer, d.h. $V$ nähert sich $V^{\\pi}$ an\n",
    "* Lernrate: Wie gut passen neue Werte die Bewertungsfunktion $V$ an?\n",
    "\n",
    "$V(s_t)\\leftarrow(1-\\alpha)V(s_t)+\\alpha (r_{t+1}  \\gamma V(s_{t+1}))$ mit Lernrate $\\alpha$ und Discount-Faktor $\\gamma$\n",
    "\n",
    "- mit Gewichtung der neuen Aktionen\n",
    "- Vorteil: neue Aktionen haben eine höhere Auswirkungen auf das Model, minimiert die Variazn im vergleich zu monte carlo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ähnlich wie bei Monte Carlo, wird Temporal difference learning zur Berechnung der Q-Matrix verwendet. Bei jedem Durchlauf entscheidet sich der Agent für die Aktion, die zu dem State mit dem höchsten Reward führt. Im Anschluss wird der Wert mit Hilfe der Value-Funktion nach folgender Formel angepasst:\n",
    "    \n",
    "$V(s_t)\\leftarrow(1-\\alpha)V(s_t)+\\alpha(r_{t+1} \\gamma V(s_{t+1}))$\n",
    "\n",
    "wobei gilt:\n",
    "* $V(s_t)$ ist der Wert des aktuellen States\n",
    "* $r_t+1$ ist der Reward, der im nächstem Schritt erhalten wird\n",
    "* $\\gamma$ ist die Gewichtung, wie hoch der Einfluss des Wertes in den Gesamtwert eingeht\n",
    "* $\\alpha$ ist die Lernrate\n",
    "\n",
    "Die Matrix optimiert sich direkt nach jedem Trainings-Durchlauf. Im Gegensatz zu Monte Carlo werden die Rewards, mit einem Gewicht $\\gamma$ versehen. Dadurch wird aktuelleren Werten eine höhere Wertigkeit im Endergebnis gegeben. Der Vorteil dabei ist, dass die Varianz minimiert wird. Ausreißer haben also kleinere Auswirkungen auf das Endergebnis.\n",
    "\n",
    "**TODO: Beispielgraph einfügen**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q-learning\n",
    "\n",
    "* Variante von TD-learning\n",
    "\n",
    "- Modellfreier R.L. Algorythmus um der Wert einer Aktion zu berechnen\n",
    "- Benötigt keine Anpassung des Enviroments, nut Rewards und stochastische Transitionen\n",
    "- Für einen Endlichen Markov Decision Prozess, findet Q-learning die Optimale Policy(Regel), um eine optimale Aktionsauswahl zu treffen, auf Basis von zufälligen Aktionen, falls man unendlich Zeit zur Verfügung hat.\n",
    "- In eine Matrix len(States)xlen(Aktionen) werden die Rewards für eine Aktion im State eingetragen.\n",
    "- Initial ist die Matrix mit 0 gefüllt\n",
    "- Die Q-Funktion: Q: S x A -> R mappt eine Aktion und ein State auf eine Rationale zahl, die in der Matrix eingetragen wird\n",
    "- Neue Werte werden mit einer höheren Gewichtung in der Matirx einberechnet (gamma, discount factor)\n",
    "- in der Tabelle sind die Werte der letzen States aufsummiert eingetragen, also die Rewards der ganzen Weges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
