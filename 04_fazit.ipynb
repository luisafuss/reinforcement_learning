{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%HTML\n",
    "<style>\n",
    ".container { width:100% }\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fazit\n",
    "**TODO: Einleitung des Fazit Kapitels**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualisierung"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Das Notebook, das im Folgenden geladen wird, beinhaltet Funktionen für die grafische Darstellung des aktuellen Zustands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ./01_einleitung.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ./02_markov_decision_process.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ./03_reinforcement_learning.ipynb\n",
    "%run ./implementierung_gui.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Folgendermaßen kann dann eine Episode ausgeführt werden. Als Methoden für die Auswahl der Aktionen sind `random_action` und `q_learning_action` möglich."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = (((3, 0), 0, 2))\n",
    "transport = Transport(state, action_method = q_learning_action, stepwise = True, visualize = True)\n",
    "update_canvas(transport)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "episode = transport.state_list\n",
    "episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mc_table = np.zeros([len(states), len(actions)])\n",
    "count_state_action = np.zeros([len(states), len(actions)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mc_function(state_list):\n",
    "    states = [s for (s, _,_) in state_list]\n",
    "    state_action_pairs = {(s, a) for (s, a,_) in state_list}\n",
    "    for (state, action) in state_action_pairs:\n",
    "        idx = states.index(state)\n",
    "        value = 0\n",
    "        for i in range (idx, len(state_list)):\n",
    "            value += state_list[i][2]\n",
    "        state_num = state_to_state_num(state)\n",
    "        count_state_action[state_num][action] += 1\n",
    "        mc_table[state_num][action] = (mc_table[state_num][action] + value)/count_state_action[state_num][action]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mc_learning(episodes):\n",
    "    epsilon = 0.3\n",
    "    \n",
    "    for episode in range(episodes):\n",
    "        state = states[random.randrange(0, len(states), 1)]\n",
    "        done  = False\n",
    "        state_list = []\n",
    "        transport  = Transport(state, None, stepwise = False, visualize = False)\n",
    "        \n",
    "        while not done:\n",
    "            state_num = state_to_state_num(transport.state)\n",
    "            if random.uniform(0, 1) < epsilon:\n",
    "                action = random.sample(possible_actions[transport.state[0]], 1)[0] # Aktionsbereich erkunden\n",
    "            else:\n",
    "                action = np.argmax(mc_table[state_num]) # Gelernte Werte ausnutzen\n",
    "                \n",
    "            reward     = transport.reward_function(action)\n",
    "            transport.state = transport.transition_function(action)\n",
    "            state_list.append((transport.state, action, reward))\n",
    "            _, position_goods, position_goal = transport.state\n",
    "            if position_goods == position_goal: done = True\n",
    "        mc_function(state_list)\n",
    "        print(str(round((episode + 1)/episodes*100, 1)) + '%', end='\\r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mc_learning(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_state_action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "Um die Performance des trainierten Agenten beurteilen zu können, wird für eine große Menge an zufälligen Startzuständen einmal zur Auswahl der Aktionen der zufällige Algorithmus genutzt und einmal jener, der auf den erlernten Werten basiert:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(number_of_tests, action_method):\n",
    "    total_value = 0\n",
    "    total_steps = 0\n",
    "\n",
    "    for i in range (number_of_tests):\n",
    "        state = states[random.randrange(0, len(states), 1)]\n",
    "        transport = Transport(state, action_method = action_method, stepwise = False, visualize = False)\n",
    "        transport.transport_goods()\n",
    "\n",
    "        total_value += transport.current_value\n",
    "        total_steps += transport.steps\n",
    "        print(round((i+1)/number_of_tests * 100, 1), '%', end='\\r')\n",
    "\n",
    "    print('Result after ' + str(number_of_tests) + ' tests:')\n",
    "    print('Average value: ' + str(total_value/number_of_tests))\n",
    "    print('Average number of steps: ' + str(total_steps/number_of_tests))\n",
    "    print('Average reward per step: ' + str((total_value/number_of_tests)/(total_steps/number_of_tests)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluation(2000, random_action)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ergebnis nach 2000 Durchläufen ohne Reinforcement Learning:\n",
    "* Average value: -1404.6395\n",
    "* Average number of steps: 890.9995\n",
    "* Average reward per step: -1.5764761933087503"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluation(2000, q_learning_action)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ergebnis nach 2000 Durchläufen mit Reinforcement Learning und 10.000 Trainingsdurchläufen für das Q-Learning:\n",
    "* Average value: 1.2275\n",
    "* Average number of steps: 12.8825\n",
    "* Average reward per step: 0.0952843004075296"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Das Ergebnis zeigt deutlich, dass sich das Trainieren gelohnt hat und sich die Anzahl der benötigten Schritte pro Episode deutlich reduziert hat. Schaut man sich die Episoden in der Visualisierung an, sieht man auch, dass der trainierte Agent meistens den optimalen, also kürzesten, Weg nimmt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO: Auswertung für Monte-Carlo und TD-Learning + Ausführlicheres Fazit**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
