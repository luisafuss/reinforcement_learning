{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%HTML\n",
    "<style>\n",
    ".container { width:100% }\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fazit\n",
    "Ziel der Arbeit war es, die benötigten Grundlagen für Reinforcement Learning zu erklären und an Hand eines Fallbeispieles die Implementierung zu erläutern.\n",
    "Zu Beginn der Arbeit wurde im Rahmen des Markov-Decision-Prozess erläutert, was für Reinforcement Learning benötigt wird. Des Weiteren wurden verschiedene Arten und Vorgehensweisen des Reinforcement Learning aufgegliedert und erklärt.\n",
    "Die verschiedenen Algorithmen zum Lernen wurden theoretisch erläutert und teilweise am Beispiel angewandt. Nun soll eine Visualisierung des Problems, des Lernverfahrens und der Lösung zum besseren Verständnis implementiert werden. \n",
    "Abschließend werden die verschiedenen Lernmodelle evaluiert."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualisierung"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zunächst werden die bereits vorgenommenen Implementierungen geladen. Das Notebook, das im als letztes geladen wird, beinhaltet Funktionen für die grafische Darstellung des aktuellen Zustands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ./02_markov_decision_process.ipynb\n",
    "%run ./03_reinforcement_learning.ipynb\n",
    "%run ./implementierung_gui.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Folgendermaßen kann dann eine Episode ausgeführt und visualisiert werden. Als Methoden für die Auswahl der Aktionen sind `random_action`, `mc_action` und `q_learning_action` möglich."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = (((3, 0), 0, 2))\n",
    "transport = Transport(state, action_method = mc_action, stepwise = True, visualize = True)\n",
    "transport.update_canvas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "Um die Performance des trainierten Agenten beurteilen zu können, wird für eine große Menge an zufälligen Startzuständen zur Auswahl der Aktionen einmal der zufällige Algorithmus genutzt und einmal jener, der auf den erlernten Werten basiert:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(number_of_tests, action_method):\n",
    "    total_value = 0\n",
    "    total_steps = 0\n",
    "\n",
    "    for i in tqdm(range(number_of_tests)):\n",
    "        state = states[random.randrange(0, len(states), 1)]\n",
    "        transport = Transport(state, action_method = action_method, stepwise = False, visualize = False)\n",
    "        transport.transport_goods()\n",
    "\n",
    "        total_value += transport.current_value\n",
    "        total_steps += transport.steps\n",
    "\n",
    "    print('Result after ' + str(number_of_tests) + ' tests:')\n",
    "    print('Average value: ' + str(total_value/number_of_tests))\n",
    "    print('Average number of steps: ' + str(total_steps/number_of_tests))\n",
    "    print('Average reward per step: ' + str((total_value/number_of_tests)/(total_steps/number_of_tests)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluation(2000, random_action)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ergebnis nach 2000 Durchläufen ohne Reinforcement Learning:\n",
    "* Average value: -1404.6395\n",
    "* Average number of steps: 890.9995\n",
    "* Average reward per step: -1.5764761933087503"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluation(2000, mc_action)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ergebnis nach 2000 Durchläufen mit Reinforcement Learning und 1.000 Trainingsdurchläufen mit der Monte-Carlo-Methode:\n",
    "* Average value: -2685.706\n",
    "* Average number of steps: 1797.876\n",
    "* Average reward per step: -1.4938215983749714"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluation(2000, q_learning_action)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ergebnis nach 2000 Durchläufen mit Reinforcement Learning und 10.000 Trainingsdurchläufen für das Q-Learning:\n",
    "* Average value: 1.2275\n",
    "* Average number of steps: 12.8825\n",
    "* Average reward per step: 0.0952843004075296"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vergleicht man die hier ermittelten Werte der Evaluation, insbesondere die durchschnittliche Belohnung pro Schritt, lässt sich eine Verbesserung von der zufälligen Auswahl der Aktionen, über die Monte-Carlo-Methode bis zur Nutzung der mittels Q-Learning berechneten Werte feststellen. Zu Beachten beim Vergleich von Monte-Carlo und Q-Learning ist, dass bei Q-Learning 10.000 Trainingsdurchgänge genutzt wurden, während es bei Monte-Carlo aus Zeitgründen nur 1.000 Iterationen gab. Das Ergebnis zeigt jedoch deutlich, dass sich das Trainieren gelohnt hat und sich die Anzahl der benötigten Schritte pro Episode bei der Verwendung von Q-Learning deutlich reduziert hat. Entsprechend ist die im Durchschnitt pro Schritt erhaltene Belohnung deutlich höher, wenn die Werte, die durch Q-Learning ermittelt wurden, genutzt werden. Die durchschnittliche Belohnung bei zufälligen Aktionen und bei Monte-Carlo liegt hingegen im negativen Bereich. Schaut man sich die Episoden in der Visualisierung an, sieht man auch, dass der mit Q-Learning trainierte Agent meistens den optimalen, also kürzesten, Weg nimmt. Eine weitere Optimierung des Algorithmus ist perspektivisch durch eine Anpassung der Werte $\\alpha$, $\\gamma$ und $\\epsilon$ möglich.\n",
    "Außerdem wurde während der Implementierung bereits das Fazit gezogen, dass Monte Carlo keine effiziente Lösung des Problems zu Verfügung stellt, da Lernvorgänge auf Grund von suboptimalen Prozessen zu lange dauern. Von den präsentierten Algorithmen ist Q-Learning für diesen Anwendungsfall zu präferieren."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
