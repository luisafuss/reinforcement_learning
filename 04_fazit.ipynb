{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%HTML\n",
    "<style>\n",
    ".container { width:100% }\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fazit\n",
    "**TODO: Einleitung des Fazit Kapitels**\n",
    "* Was haben wir gemacht? Zusammenfassung\n",
    "* Ziel war es reinforcement learning zu erklären mit benötigten Grundlagen erklären an hand eines Fallbeispieles mit Implementierung\n",
    "* Zu Beginn der arbeit wurden die Grundlagen erläutert (Markov). Des Weiteren wurde auf Reinforcement Learning eingegangen, Aufgliederung in verschiedene arten  und Vorgehensweisen des RL\n",
    "* Die verschiedenen Algorithmen zum Lernen wurden theoretisch erläutert und teilweise am Beispiel angewendet. Visualisierung des Problems, des der Lernverfahrens und der Lösung. \n",
    "*  Abschließend sollen die verschienenen Lernmodelle evaluiert werden."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualisierung"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Das Notebook, das im Folgenden geladen wird, beinhaltet Funktionen für die grafische Darstellung des aktuellen Zustands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ./01_einleitung.ipynb\n",
    "%run ./02_markov_decision_process.ipynb\n",
    "%run ./03_reinforcement_learning.ipynb\n",
    "%run ./implementierung_gui.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Folgendermaßen kann dann eine Episode ausgeführt werden. Als Methoden für die Auswahl der Aktionen sind `random_action` und `q_learning_action` möglich."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = (((3, 0), 0, 2))\n",
    "transport = Transport(state, action_method = q_learning_action, stepwise = True, visualize = True)\n",
    "transport.update_canvas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "Um die Performance des trainierten Agenten beurteilen zu können, wird für eine große Menge an zufälligen Startzuständen einmal zur Auswahl der Aktionen der zufällige Algorithmus genutzt und einmal jener, der auf den erlernten Werten basiert:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(number_of_tests, action_method):\n",
    "    total_value = 0\n",
    "    total_steps = 0\n",
    "\n",
    "    for i in range (number_of_tests):\n",
    "        state = states[random.randrange(0, len(states), 1)]\n",
    "        transport = Transport(state, action_method = action_method, stepwise = False, visualize = False)\n",
    "        transport.transport_goods()\n",
    "\n",
    "        total_value += transport.current_value\n",
    "        total_steps += transport.steps\n",
    "        print(round((i+1)/number_of_tests * 100, 1), '%', end='\\r')\n",
    "\n",
    "    print('Result after ' + str(number_of_tests) + ' tests:')\n",
    "    print('Average value: ' + str(total_value/number_of_tests))\n",
    "    print('Average number of steps: ' + str(total_steps/number_of_tests))\n",
    "    print('Average reward per step: ' + str((total_value/number_of_tests)/(total_steps/number_of_tests)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluation(2000, random_action)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ergebnis nach 2000 Durchläufen ohne Reinforcement Learning:\n",
    "* Average value: -1404.6395\n",
    "* Average number of steps: 890.9995\n",
    "* Average reward per step: -1.5764761933087503"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluation(2000, q_learning_action)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ergebnis nach 2000 Durchläufen mit Reinforcement Learning und 10.000 Trainingsdurchläufen für das Q-Learning:\n",
    "* Average value: 1.2275\n",
    "* Average number of steps: 12.8825\n",
    "* Average reward per step: 0.0952843004075296"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Das Ergebnis zeigt deutlich, dass sich das Trainieren gelohnt hat und sich die Anzahl der benötigten Schritte pro Episode deutlich reduziert hat. Schaut man sich die Episoden in der Visualisierung an, sieht man auch, dass der trainierte Agent meistens den optimalen, also kürzesten, Weg nimmt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO: Auswertung für random, qlearning, monte carlo Ausführlicheres Fazit**\n",
    "\n",
    "* Auf die Werte des Auswertung einzelt eingehen\n",
    "* Anpassung von Alpha gamma epsilon eine optimierung vonnehmen\n",
    "* Monte Carlo ist für das Problem nicht gut geeignet, weil der Trainingsvorgang zu lange dauert.\n",
    "* Q-learning bietet eine ausreichende Lösungsansatz "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
