{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementierung eines Reinforcement-Learning-Projekts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%HTML\n",
    "<style>\n",
    ".container { width:100% }\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Das Problem\n",
    "Wir sind Betreiber eine Spedition und sind zuständig für die Belieferung von Supermärkten. Dafür muss Ware zwischen Lagern und den Märkten innerhalb unserer Stadt transportiert werden. Unser innovativer LKW ist selbstfahrend und bekommt lediglich den Auftrag an einer Station (Lager oder Supermarkt) Ware einzusammeln und an einer anderen Station wieder abzuliefern."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definitionen\n",
    "Wir bedienen zwei Lager und zwei Supermärkte.\n",
    "\n",
    "Lager:\n",
    "0. A-Lager (A)\n",
    "1. B-Lager (B)\n",
    "\n",
    "Supermarkt:\n",
    "2. C-Markt (C)\n",
    "3. D-Markt (D)\n",
    "\n",
    "Diese verteilen sich folgendermaßen in unserer Stadt:\n",
    "\n",
    "<img src=\"img/city.png\" alt=\"stadt\" width=\"400\"/>\n",
    "\n",
    "Die Stadt ist eine 6x6-Quadratestadt und mit 36 Positionen, die die Koordinaten $(0,0)$ bis $(5,5)$ haben. Der LKW kann sich frei in der Stadt bewegen, aber nicht durch die Grünstreifen fahren.\n",
    "\n",
    "Die Anzahl der Zustände ergibt sich folgendermaßen:\n",
    "* 6 x 6 Positionen\n",
    "* 4 Orte zu denen die Ware gebracht werden kann (A bis D bzw. 0 bis 3)\n",
    "* 5 Orte, an denen sich die Ware befindet (A bis D bzw. 0 bis 3 und im LKW (Position Nr.4))\n",
    "\n",
    "\\\\[ 6 \\cdot 6 \\cdot 4 \\cdot 5 = 720 \\texttt{ mögliche Zustände}\\\\]\n",
    "\n",
    "Die Aktionen, die der LKW ausführen kann sind:\n",
    "0. nach Norden fahren\n",
    "1. nach Osten fahren\n",
    "2. nach Süden fahren\n",
    "3. nach Westen fahren\n",
    "4. Ware einsammeln\n",
    "5. Ware abladen\n",
    "\n",
    "Dabei kann er folgende Belohnungen (und Abzüge) erhalten:\n",
    "* Ware korrekt abliefern: +20\n",
    "* Ware falsch einsammeln/abliefern: -10\n",
    "* Pro Schritt: -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definitionen implementieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import random\n",
    "import time\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Im Folgenden werden die zuvor getroffenen Spezifikationen in ein für den Rechner verständliches Format überführt. Es gibt jeweils sechs Spalten und Reihen (0 bis 5) und eine Menge von Aktionen (ebenfalls 0 bis 5). Für die Ausgabe wird noch eine Überführung in eine Beschreibung vorgenommen. Außerdem werden die Koordinaten der Lager und Supermärlte festgehalten. Die Koordianten haben dabei die Form `(Reihe, Spalte)`.\n",
    "\n",
    "Die Grünstreifen werden in einer Menge gespeichert. Ein Grünstreifen liegt immer zwischen zwei Feldern. Diese werden in einem Tupel in der Reihenfolge `(Links, Rechts)` bzw. `(Oben, Unten)` angegeben. Daraus ergibt sich für jedes Stück eines Grünstreifens ein Tupel der Form `((Reihe Zelle links, Spalte Zelle links), (Reihe Zelle rechts, Spalte Zelle rechts))` bzw. mit \"oben\" und \"unten\", falls es sich um einen horizontalen Streifen handelt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = [row for row in range(0, 6)]\n",
    "cols = [col for col in range(0, 6)]\n",
    "actions = {action for action in range(0, 6)}\n",
    "actions_description = [\"Drive north\", \"Drive east\", \"Drive south\", \"Drive west\", \"Pickup goods\", \"Dropoff goods\"]\n",
    "stations = [(0,0), (0,3), (5,0), (4,5)]\n",
    "stations_descriptions = [\"Warehouse A\", \"Warehouse B\", \"Supermarket C\", \"Supermarket D\"]\n",
    "position_goods_descriptions = stations_descriptions + [\"In the truck\"]\n",
    "walls = {\n",
    "    ((0,2), (0,3)), #vertikal\n",
    "    ((1,2), (1,3)),\n",
    "    ((3,1), (3,2)),\n",
    "    ((4,0), (4,1)),\n",
    "    ((5,0), (5,1)),\n",
    "    ((5,2), (5,3)),\n",
    "    ((1,0), (2,0)), # horizonal\n",
    "    ((1,5), (2,5)),\n",
    "    ((3,4), (4,4)),\n",
    "    ((3,5), (4,5))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Als nächstes soll für jedes Feld im Stadtplan festgehalten werden, welche Aktionen von diesem Feld ausgehend möglich sind. Zunächst wird allen Feldern alle Aktionen zugewiesen. Den am Rand liegenden Feldern wird die Aktion aberkannt, die aus der Stadt heraus führen würde. Einsammel- und Ablieferaktionen sind nur an den zuvor definierten Stationen möglich, weshalb den anderen Feldern diese Aktion ebenfalls entzogen wird. Als nächstes werden Felder betrachtet, die in der direkten Nachbarschaft eines Grünstreifens liegen und dort die Aktionen entfernt, die den LKW dazu veranlassen würden, die Grünanlage zu zerstören. Daraus ergibt sich dann das gewünschte Dictionary mit dem feld als Key und einer Menge möglicher Aktionen als Value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "possible_actions = dict()\n",
    "\n",
    "for row in rows:\n",
    "    for col in cols:\n",
    "        possible_actions[(row, col)] = copy.deepcopy(actions)\n",
    "for key in possible_actions:\n",
    "    (row, col) = key\n",
    "    # Ränder\n",
    "    if row == 0:\n",
    "        possible_actions[key].remove(0)\n",
    "    elif row == 5:\n",
    "        possible_actions[key].remove(2)\n",
    "    if col == 0:\n",
    "        possible_actions[key].remove(3)\n",
    "    elif col == 5:\n",
    "        possible_actions[key].remove(1)\n",
    "    # Einsammeln, Abliefern\n",
    "    if (row, col) not in stations:\n",
    "        possible_actions[key].remove(4)\n",
    "        possible_actions[key].remove(5)\n",
    "    # Grünstreifen\n",
    "    if ((row, col), (row, col + 1)) in walls:\n",
    "        possible_actions[key].remove(1)\n",
    "    if ((row, col - 1), (row, col)) in walls:\n",
    "        possible_actions[key].remove(3)\n",
    "    if ((row, col), (row + 1, col)) in walls:\n",
    "        possible_actions[key].remove(2)\n",
    "    if ((row - 1, col), (row, col)) in walls:\n",
    "        possible_actions[key].remove(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Die Transport-Klasse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Transportklasse speichert den aktuellen Zustand (`state`), die grafische Darstellung (`canvas`) von diesem sowie den aktuell erreichten Wert (`current_value`). Außerdem speichert sie, wie viele Schritte bereits ausgeführt wurden (`steps`). Ebenfalls ist eine Methode zur Ermittlung der nächsten Aktion, die `action_method`, zu übergeben. Dies kann eine Methode sein, die eine zufällige Aktion zurückgibt (`random_action`) oder eine Methode, die die durch Q-Learning erlernten Werte ausnutzt (`q_learning_action`). Außerdem werden alle besuchten Zustände in `visited_states` aufgelistet. Ist die Variable `stepwise` auf `True` gesetzt, wird zur besseren Sichtbarkeit der Aktionen in der Darstellung nach jeder Aktion eine kurze Pause durchgeführt. Die graphische Darstellung erfolgt nur, wenn `visualize` auch `True` ist. Die Variable `done` markiert, ob der Transport der Waren erfolgreich abgeschlossen wurde. Für eine spätere Berechnung wird die Vorgängerposition der Waren benötigt, weshalb diese hier ebenfalls abgespeichert wird."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transport():\n",
    "    def __init__(self, state, action_method, stepwise = False, visualize = False):\n",
    "        self.state = state\n",
    "        self.canvas = init_canvas(self.state)\n",
    "        self.action_method = action_method\n",
    "        self.current_value = 0\n",
    "        self.steps = 0\n",
    "        self.visited_states = [self.state]\n",
    "        self.stepwise = stepwise\n",
    "        self.visualize = visualize\n",
    "        self.pause = True\n",
    "        self.done = False\n",
    "        self.position_goods_old = self.state[1]\n",
    "        \n",
    "        self.canvas[3].on_mouse_down(self.handle_mouse_down)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Transition function gibt für eine Aktion den Zustand zurück der folgt, wenn man auf den aktuellen Zustand die gewünschte Aktion anwendet. Falls die Aktion nicht möglich ist, wird der alte Zustand zurückgegeben. Ein Zustand ist dabei folgendermaßen aufgebaut:\n",
    "\n",
    "`(Position LKW, Position Ware, Position Ziel)`.\n",
    "\n",
    "Wobei gilt:\n",
    "* Position LKW = (Reihe LKW, Spalte LKW)\n",
    "* Position Ware $\\in$ \\[0, 4\\] $\\rightarrow$ \\[Lager A, Lager B, Supermarkt C, Supermarkt D, im LKW\\]\n",
    "* Position Ziel $\\in$ \\[0, 3\\] $\\rightarrow$ \\[Lager A, Lager B, Supermarkt C, Supermarkt D\\]\n",
    "\n",
    "Bei der Funktion handelt es sich um eine Funktion der Klasse `Transport`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transition_function(self, action):\n",
    "    position_truck, position_goods, position_goal = self.state\n",
    "    if action not in possible_actions[position_truck]:\n",
    "        return self.state\n",
    "    \n",
    "    row, col = position_truck\n",
    "    if action == 0:\n",
    "        position_truck = (row - 1, col)\n",
    "    elif action == 1:\n",
    "        position_truck = (row, col + 1) \n",
    "    elif action == 2:\n",
    "        position_truck = (row + 1, col)\n",
    "    elif action == 3:\n",
    "        position_truck = (row, col - 1)\n",
    "    elif action == 4:\n",
    "        if position_goods == stations.index(position_truck):\n",
    "            position_goods = 4\n",
    "    elif action == 5:\n",
    "        if position_goods != 4:\n",
    "            return self.state\n",
    "        position_goods = stations.index(position_truck)\n",
    "            \n",
    "    return (position_truck, position_goods, position_goal) \n",
    "Transport.transition_function = transition_function\n",
    "del transition_function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`reward_function()` gibt die Belohnung zurück, die furch die Aktion im aktuellen Zustand erzielt wird.\n",
    "Bei der Funktion handelt es sich ebenfalls um eine Funktion der Klasse `Transport`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reward_function(self, action):\n",
    "    reward = -1 # Pro Schritt\n",
    "    position_truck, position_goods, position_goal = self.state\n",
    "    \n",
    "    if action not in possible_actions[position_truck]: return reward\n",
    "    \n",
    "    if action == 4:\n",
    "        station_truck = stations.index(position_truck)\n",
    "        if position_goods != station_truck: # Ware falsch einsammeln\n",
    "            reward -= 10\n",
    "    elif action == 5:\n",
    "        station_truck = stations.index(position_truck)\n",
    "        if position_goal != station_truck and position_goods == 4: # Ware falsch abliefern  \n",
    "            reward -= 10\n",
    "        elif position_goal == station_truck and position_goods == 4: # Ware korrekt abliefern\n",
    "            reward += 20\n",
    "        else: # Abliefern ohne geladene Ware\n",
    "            reward -= 10\n",
    "            \n",
    "    return reward\n",
    "Transport.reward_function = reward_function\n",
    "del reward_function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Funktion `random_action()` gibt eine zufällige Aktion zurück."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_action(state_num):\n",
    "    return random.randrange(0, 6, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`handle_mouse_down()` wird durch Klicken auf die Karte aufgerufen. Die Funktion ruft lediglich `transport_goods()` auf. Diese Funktion führt dann so oft eine Aktion, die von der gewünschten `action_method()` ausgewählt wird, aus, bis die Ware an ihrem Zielort eingetroffen ist. Nach dem Ausführen wird die Darstellung falls gewünscht aktualisiert. Die Konstante `MAX_VISITS_STATE` gibt an, wie oft ein Zustand besucht werden kann, bevor ausgehend von diesem Zustand statt der eigentlich gewünschten Aktion eine zufällige Aktion gestartet wird. Diese Randomisierung ist notwendig, um zu verhindern, dass sich Aktions-Schleifen bilden, in denen der Agent festhängt. Im idealen Fall besucht der Agenten einen State nur ein mal (Position zweimal, einmal mit Paktet, einmal ohne). DEr Wert wird trotzdem höher angesetzt, um keine zu starke Einschränkung zu bieten.\n",
    "\n",
    "Bei der Funktion handelt es sich auch um eine Funktion der Klasse `Transport`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_mouse_down(self, x, y):\n",
    "    self.transport_goods()\n",
    "Transport.handle_mouse_down = handle_mouse_down\n",
    "del handle_mouse_down"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_VISITS_STATE = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transport_goods(self):\n",
    "    position_truck, position_goods, position_goal = self.state\n",
    "    while not self.done:    \n",
    "        self.steps += 1\n",
    "        state_num = state_to_state_num(self.state)\n",
    "        action = self.action_method(state_num)\n",
    "        if action not in possible_actions[position_truck]:\n",
    "            action = random.sample(possible_actions[position_truck], 1)[0]\n",
    "        \n",
    "        value = self.reward_function(action)\n",
    "        new_state = self.transition_function(action)\n",
    "        if self.visited_states.count(new_state) > MAX_VISITS_STATE:\n",
    "            action = random.sample(possible_actions[position_truck], 1)[0]\n",
    "            value  = self.reward_function(action)\n",
    "            new_state = self.transition_function(action)\n",
    "        \n",
    "        self.current_value += value\n",
    "        self.state = new_state\n",
    "        self.visited_states.append(self.state)\n",
    "        position_truck, position_goods, position_goal = self.state\n",
    "        \n",
    "        if position_goods == position_goal: self.done = True\n",
    "        if self. visualize:\n",
    "            action_label.value = 'Action: ' + actions_description[action]\n",
    "            update_canvas(self)\n",
    "        if self.stepwise: time.sleep(0.1)\n",
    "Transport.transport_goods = transport_goods\n",
    "del transport_goods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Im folgenden werden alle möglichen Zustände in einer Liste gespeichert, sodass sie nummerierbar und damit eindeutig identifizierbar sind. Die nachfolgenden Funktionen `state_to_state_num()` und `state_num_to_state()` dienen dazu, zwischen den beiden Darstellungsweisen zu wechseln."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states = []\n",
    "for row in range (0, 6):\n",
    "    for col in range (0, 6):\n",
    "        for position_goods in range (0, 5):\n",
    "            for position_goal in range (0, 4):\n",
    "                states.append(((row, col), position_goods, position_goal))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def state_to_state_num(state):\n",
    "    return states.index(state)\n",
    "\n",
    "def state_num_to_state(num):\n",
    "    return states[num]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zunächst wird eine Tabelle mit einer Zeile für jeden Zustand und einer Spalte für jede Aktion angelegt. In jede Zelle wird zu Beginn der Wert Null geschrieben."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "q_table = np.zeros([len(states), len(actions)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die `q_function()` nimmt einen Zustand und eine Aktion sowie die Lernrate $\\alpha$ und den Discount-Faktor $\\gamma$. Darauf basierend berechnet sie mit Hilfe der `q_table` den neuen Wert für den Zustand und die Aktion und trägt diesen an der entsprechenden Stelle in der Tabelle ein. Zurückgegeben wird außerdem der nächste Zustand. Die Formel für die Berechnung des neuen Wertes lautet folgendermaßen:\n",
    "\n",
    "\\\\[Q_{\\texttt{new}}(s_t, a_t) \\leftarrow Q_{\\texttt{old}}(s_t, a_t) + \\alpha \\cdot (r_t + \\gamma \\cdot max_q(s{t+1}, a) - Q_{\\texttt{old}}(s_t, a_t))\\\\]\n",
    "\n",
    "mit:\n",
    "* $s_t$, $s_{t+1} \\in$ len(states)\n",
    "* a $\\in$ actions\n",
    "* $Q_{\\texttt{old}}(s_t, a_t)$ = Tabellenwert in Zeile $s_t$ und Spalte $a_t$\n",
    "* $\\alpha$ = Lernrate\n",
    "* $\\gamma$ = Discount-Faktor\n",
    "* $r_t$ = Reward\n",
    "* max_q = Schätzung des optimalen Zukunftswertes\n",
    "\n",
    "Der Wert in der Klammer wird dabei auch als \"temporal difference\" bezeichnet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_function(state, action, alpha, gamma):\n",
    "    transport  = Transport(state, stepwise = False, visualize = False)\n",
    "    \n",
    "    reward     = transport.reward_function(action)\n",
    "    next_state = transport.transition_function(action)\n",
    "    \n",
    "    state_num      = state_to_state_num(state)\n",
    "    next_state_num = state_to_state_num(next_state)\n",
    "    \n",
    "    old_value  = q_table[state_num][action]\n",
    "    max_q      = np.max(q_table[next_state_num])\n",
    "    new_value  = old_value + alpha * (reward + gamma * max_q - old_value)\n",
    "    \n",
    "    q_table[state_num, action] = new_value\n",
    "    return next_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Funktion `q_learning()` führt für eine zu übergebende Anzahl an Startzuständen (`episodes`) den Transport der Waren zum Ziel durch. Der Startzustand ist dabei ein zufälliger. Dies dient dem Training und somit der Verbesserung der `q_table`. Denn in jedem Schritt wird `q_function()` aufgerufen.  In rund 10% (= `epsilon`) der Schritte ist auch die Aktion zufällig ausgewählt. In den anderen Fällen wird die am besten bewertete Aktion für den aktuellen Zustand genutzt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_learning(episodes):\n",
    "    alpha   = 0.1\n",
    "    gamma   = 0.6\n",
    "    epsilon = 0.1\n",
    "    \n",
    "    for episode in range(episodes):\n",
    "        state = states[random.randrange(0, len(states), 1)]\n",
    "        done  = False\n",
    "        \n",
    "        while not done:\n",
    "            state_num = state_to_state_num(state)\n",
    "            if random.uniform(0, 1) < epsilon:\n",
    "                action = random.sample(possible_actions[state[0]], 1)[0] # Aktionsbereich erkunden\n",
    "            else:\n",
    "                action = np.argmax(q_table[state_num]) # Gelernte Werte ausnutzen\n",
    "                \n",
    "            state = q_function(state, action, alpha, gamma)\n",
    "            _, position_goods, position_goal = state\n",
    "            if position_goods == position_goal: done = True\n",
    "        print(round(episode + 1/episodes*100, 1), '%', end='\\r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zum Trainieren muss die Funktion `q_learning()` mit einem relativ hohen Wert aufgerufen werden. Damit dies nicht nach jedem Neustart des Kernels notwendig ist, kann die Tabelle abgespeichert und beim nächsten mal einfach gelade werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#q_learning(10000) # Trainieren\n",
    "#np.savetxt('q_table.txt', q_table) # Speichern der Tabelle\n",
    "q_table = np.loadtxt('q_table.txt') # Laden der Tabelle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die folgende Methode kann dann nach dem Trainieren in der tatsächlichen Anwendung genutzt werden, üm für jeden Zustand die beste Aktion zu ermitteln. Dazu muss sie der Klasse `Transport` bei der Initialisierung übergeben werden und wird später dann von `transport_goods()` aufgerufen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_learning_action(state_num):\n",
    "    return np.argmax(q_table[state_num]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualisierung"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Das Notebook, das im Folgenden geladen wird, beinhaltet Funktionen für die grafische Darstellung des aktuellen Zustands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ./04_implementierung_gui.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Folgendermaßen kann dann eine Episode ausgeführt werden. Als Methoden für die Auswahl der Aktionen sind `random_action` und `q_learning_action` möglich."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#state = (((3, 0), 0, 2))\n",
    "#transport = Transport(state, action_method = q_learning_action, stepwise = True, visualize = True)\n",
    "#update_canvas(transport)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "Um die Performance des trainierten Agenten beurteilen zu können, wird für eine große Menge an zufälligen Startzuständen einmal zur Auswahl der Aktionen der zufällige Algorithmus genutzt und einmal jener, der auf den erlernten Werten basiert:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(number_of_tests, action_method):\n",
    "    total_value = 0\n",
    "    total_steps = 0\n",
    "\n",
    "    for i in range (number_of_tests):\n",
    "        state = states[random.randrange(0, len(states), 1)]\n",
    "        transport = Transport(state, action_method = action_method, stepwise = False, visualize = False)\n",
    "        transport.transport_goods()\n",
    "\n",
    "        total_value += transport.current_value\n",
    "        total_steps += transport.steps\n",
    "        #if i%10 == 0:\n",
    "        print(round((i+1)/number_of_tests * 100, 1), '%', end='\\r')\n",
    "\n",
    "    print('Result after ' + str(number_of_tests) + ' tests:')\n",
    "    print('Average value: ' + str(total_value/number_of_tests))\n",
    "    print('Average number of steps: ' + str(total_steps/number_of_tests))\n",
    "    print('Average reward per step: ' + str((total_value/number_of_tests)/(total_steps/number_of_tests)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation(2000, random_action)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ergebnis nach 2000 Durchläufen ohne Reinforcement Learning:\n",
    "* Average value: -1404.6395\n",
    "* Average number of steps: 890.9995\n",
    "* Average reward per step: -1.5764761933087503"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation(2000, q_learning_action)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ergebnis nach 2000 Durchläufen mit Reinforcement Learning und 10.000 Trainingsdurchläufen für das Q-Learning:\n",
    "* Average value: 1.2275\n",
    "* Average number of steps: 12.8825\n",
    "* Average reward per step: 0.0952843004075296"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Das Ergebnis zeigt deutlich, dass sich das Trainieren gelohnt hat und sich die Anzahl der benötigten Schritte pro Episode deutlich reduziert hat. Schaut man sich die Episoden in der Visualisierung an, sieht man auch, dass der trainierte Agent meistens den optimalen, also kürzesten, Weg nimmt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
