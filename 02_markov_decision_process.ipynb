{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Markov Decision Process\n",
    "* Was ist das, warum behandeln wird das?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment, states, actions und rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Der Prozess, nach dem bei den meisten Reinforcement Learning Problemen vorgegangen wird, ist das Marcov Desicion Prozess. Er gibt an, wie Entscheidungen getroffen werden sollen. MDP Versucht Probleme innerhalb einer Umgebung zu erfassen.\n",
    "\n",
    "**States** sind eine Menge an Token, die angeben, in welchem Zustand sich jemand in der Umgebung befindet. Dies kann zum Beispiel die Position sein, in der sich jemand in der Umgebung befindet. (im Beispiel 12 verschiedene Zustände) \n",
    "Ein **Agent** kann in einer **Umgebung** (Enviroment e) fest definierte Menge an **Aktionen** ausführen, die ihn in einen neuen State bringen. \n",
    "\n",
    "Die **Transistion-Funktion**, auch Model genannt, T(s,a,s’) bekommt einen State s, eine Aktion a und einen weiteren State s’ und berechnet die Wahrscheinlichkeit, dass die Aktion a im State s den Agenten in den State s’ überführt. (wie oft welche aktionen ausgeführt werden ist definiert) (terministisch, unterministisch)\n",
    "Das Model genügt zwei Eigenschaften:\n",
    "1. Das Markovische Prinzip besagt, dass die vergangenen States oder Aktionen keinen EInfluss auf die aktuelle oder zukünftigen Entscheidungen haben. Die Wahrscheinlichkeit, die die Transition-Funktion berechnet ist unabhängig von den vorherigen States.\n",
    "2. Die Berechnung, die die Transitionsfunktion durchführt, ändert sich nicht.\n",
    "\n",
    "Alle Schritte, die bis jetzt beschrieben wurden, befassen sich mit dem Problem. \n",
    "Um eine Lösung zu finden, wird eine **Policy-Funktion** pi(s) : a aufgestellt, die für einen State s eine Aktion zurückgibt, die die Höchste Belohnung für den Agenten vorhersagt. Die Policy-Funktion kennt Tripel der Form <s,a,r> wobei s der aktuelle State ist, a die Aktion die im State s ausgeführt wird und r die Belohnung, die der Agent für die Aktion a im State s bekommen würde. Die Policy-Funktion hat viele von diesen Tripeln vorliegen und gibt die Aktion mit der höchsten Belohnung zurück.\n",
    "Eine optimale Policy pi* gibt immer die Aktion an, die langfristig die höchste Belohnung ergibt.\n",
    "\n",
    "Weiter: policies \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transition function und reward function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
