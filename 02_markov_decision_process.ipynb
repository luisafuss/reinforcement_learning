{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Markov Decision Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Grundlage für Reinforcement learning bilden Markow-Entscheidungsprozesse. Aus diesem Grund sollen jene nun genauer betrachtet werden.\n",
    "Ein Markov Decision Process (MDP) oder zu Deutsch auch Markow-Entscheidungsprozess ist ein Fünf-Tupel der Form $(S, A, T, R, s_0) $\n",
    "wobei gilt:\n",
    "\n",
    "* $S$ = eine Menge von Zuständen (states)\n",
    "* $A$ = eine Menge von Aktionen (actions)\n",
    "* $T(s, a, s’)$ = eine transition function mit $s \\in S$ \n",
    "* $R$ = Belohnungsfunktion $R(s, a, s')$\n",
    "* $s_0$ = Startzustand\n",
    "\n",
    "Ein MDP ist immer von seinem Umfeld, dem Environment abhänging und kann optional einen Endzustand besitzen. Die Lösung von einem solchen Problem ist eine Funktion der Form $\\pi \\colon S\\rightarrow A$. Diese wird auch Policy genannt. Die Policy gibt an, welche Aktion in welchem Zustand ausgeführt wird. Ziel ist es, eine (möglichst) optimale Policy zu finden. (vgl. Abbeel und Klein)\n",
    "\n",
    "Der Prozess hat dabei die Markov-Eigenschaft. Das bedeutet, dass der Folgezustand nur vom aktuellen Zustand abhänging ist und nicht auf den vorangegangenen basiert. Es gilt also:\n",
    "\n",
    "$P(S_{t+1}=s_t'|s_t,a_t,s_{t-1},a_{t-1})  =  P(S_{t+1}=s_t'|s_t,a_t) \\forall s \\in S$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment, states, actions und rewards\n",
    "Der Prozess, nach dem bei den meisten Reinforcement Learning Problemen vorgegangen wird, ist das Marcov Desicion Prozess. Er gibt an, wie Entscheidungen getroffen werden sollen. MDP Versucht Probleme innerhalb einer Umgebung zu erfassen.\n",
    "\n",
    "**States** sind eine Menge an Token, die angeben, in welchem Zustand sich jemand in der Umgebung befindet. Dies kann zum Beispiel die Position sein, in der sich jemand in der Umgebung befindet. (im Beispiel 12 verschiedene Zustände) Ein **Agent** kann in einer **Umgebung** (Enviroment e) fest definierte Menge an **Aktionen** ausführen, die ihn in einen neuen State bringen.\n",
    "\n",
    "Die **Transistion-Funktion**, auch Model genannt, T(s,a,s’) bekommt einen State s, eine Aktion a und einen weiteren State s’ und berechnet die Wahrscheinlichkeit, dass die Aktion a im State s den Agenten in den State s’ überführt. (wie oft welche aktionen ausgeführt werden ist definiert) (terministisch, unterministisch) Das Model genügt zwei Eigenschaften:\n",
    "\n",
    "1. Das Markovische Prinzip besagt, dass die vergangenen States oder Aktionen keinen EInfluss auf die aktuelle oder zukünftigen Entscheidungen haben. Die Wahrscheinlichkeit, die die Transition-Funktion berechnet ist unabhängig von den vorherigen States.\n",
    "2. Die Berechnung, die die Transitionsfunktion durchführt, ändert sich nicht.\n",
    "\n",
    "Alle Schritte, die bis jetzt beschrieben wurden, befassen sich mit dem Problem. Um eine Lösung zu finden, wird eine Policy-Funktion pi(s) : a aufgestellt, die für einen State s eine Aktion zurückgibt, die die Höchste Belohnung für den Agenten vorhersagt. Die **Policy-Funktion** kennt Tripel der Form <s,a,r> wobei s der aktuelle State ist, a die Aktion die im State s ausgeführt wird und r die Belohnung, die der Agent für die Aktion a im State s bekommen würde. Die Policy-Funktion hat viele von diesen Tripeln vorliegen und gibt die Aktion mit der höchsten Belohnung zurück. Eine optimale Policy pi* gibt immer die Aktion an, die langfristig die höchste Belohnung ergibt.\n",
    "\n",
    "Weiter: policies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transition function, Probability und Reward function\n",
    "Die Transition function gibt an, mit welcher Wahrscheinlichkeit man von einem Zustand in den nächsten gelangt. Es handelt sich also um eine Funktion der Art  $P_{ss'}(s'| s, a)$. Dabei is $P$ die Wahrscheinlichkeit, dass Aktion $a$ vom Zustand $s$ zu $s'$ führt). Das Ergebnis der Funktion kann mit Hilfe einer Matrix dargestellt werden:\n",
    "\n",
    "\n",
    "$P = \\begin{bmatrix}\n",
    "P_{11} & ... & P_{1n}\\\\\n",
    "... &  &\\\\\n",
    "P_{n1} & ... & P_{nn}\n",
    "\\end{bmatrix}$ wobei $1$ bis $n$ alle möglichen Zustände bezeichnen\n",
    "\n",
    "Die Summe der Werte aus einer Reihe der Matrix muss für alle Reihen 1 ergeben.\n",
    "\n",
    "Für jeden Zustandswechsel kann eine Belohnung $r$ (\"Reward\") vergeben werden. Der Höhe der Belohung ist durch die Reward function festgelegt. In vielen MDPs wird die Belohnung mit einem Faktor $\\gamma$, dem \"Discount\" multipliziert. Dabei ist $\\gamma \\in [0,1]$ und wird mit der Anzahl der bisher erfolgten Schritte $t$ potenziert. Der Discount-Faktor wird oftmals genutzt, um zu erreichen, das früher erhaltene Belohnungen stärker gewichtet werden als spätere. Der aktuelle Wert des Zustands ergibt sich aus der Summe der erhaltenen Belohungen mal des Discounts. Ziel ist es, den Wert zu maximieren.\n",
    "Es gilt:\n",
    "\n",
    "\\\\[\\texttt{Wert} = \\sum_{k = 0}^t \\gamma^k\\cdot r_{k+1}\\\\]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
