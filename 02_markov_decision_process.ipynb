{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%HTML\n",
    "<style>\n",
    ".container { width:100% }\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Markov Decision Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Grundlage für Reinforcement Learning bilden Markow-Entscheidungsprozesse. Aus diesem Grund sollen jene nun genauer betrachtet werden.\n",
    "Ein Markov Decision Process (MDP) oder zu Deutsch auch Markow-Entscheidungsprozess ist ein Fünf-Tupel der Form $(S, A, T, R, s_0) $\n",
    "wobei gilt:\n",
    "\n",
    "* $S$ = eine Menge von Zuständen (states)\n",
    "* $A$ = eine Menge von Aktionen (actions)\n",
    "* $T(s, a, s’)$ = eine transition function mit $s \\in S$ \n",
    "* $R$ = Belohnungsfunktion $R(s, a, s')$\n",
    "* $s_0$ = Startzustand\n",
    "\n",
    "Ein MDP ist immer von seinem Umfeld, dem Environment abhängig und kann optional einen Endzustand besitzen. Die Lösung von einem solchen Problem ist eine Funktion der Form $\\pi \\colon S\\rightarrow A$. Diese wird auch Policy genannt. Die Policy gibt an, welche Aktion in welchem Zustand ausgeführt wird. Ziel ist es, eine (möglichst) optimale Policy zu finden. (vgl. Abbeel und Klein 2014 a)\n",
    "\n",
    "$States$ sind eine Menge an Token, die angeben, in welchem Zustand sich jemand in der Umgebung befindet. Dies kann zum Beispiel die Position sein, in der sich jemand, bzw. genauer gesagt der Agent, in der Umgebung befindet. Ein $Agent$ kann in einer Umgebung ($Environment$ $e$) eine fest definierte Menge an $Aktionen$ ausführen, die ihn in einen neuen $State$ $s$ bringen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Diese Theorie soll nun praktisch an einem Bespiel von einem Lieferwagen eingesetzt und erläutert werden, bevor näher auf die Transition- und Reward-Function eingegangen wird."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementierung 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Das Problem\n",
    "Wir sind Betreiber einer Spedition und sind zuständig für die Belieferung von Supermärkten. Dafür muss Ware zwischen Lagern und den Märkten innerhalb unserer Stadt transportiert werden. Unser innovativer LKW ist selbstfahrend und bekommt lediglich den Auftrag an einer Station (Lager oder Supermarkt) Ware einzusammeln und an einer anderen Station wieder abzuliefern."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definitionen\n",
    "Wir bedienen zwei Lager und zwei Supermärkte.\n",
    "\n",
    "Lager:\n",
    "0. A-Lager (A)\n",
    "1. B-Lager (B)\n",
    "\n",
    "Supermarkt:\n",
    "2. C-Markt (C)\n",
    "3. D-Markt (D)\n",
    "\n",
    "Diese verteilen sich folgendermaßen in unserer Stadt:\n",
    "\n",
    "<img src=\"img/city.png\" alt=\"stadt\" width=\"400\"/>\n",
    "\n",
    "Die Stadt ist eine 6x6-Quadratestadt und mit 36 Positionen, die die Koordinaten $(0,0)$ bis $(5,5)$ haben, versehen. Der LKW kann sich frei in der Stadt bewegen, aber nicht durch die Grünstreifen fahren.\n",
    "\n",
    "Die Anzahl der Zustände ($States$) ergibt sich folgendermaßen:\n",
    "* 6 x 6 Positionen\n",
    "* 4 Orte zu denen die Ware gebracht werden kann (A bis D bzw. 0 bis 3)\n",
    "* 5 Orte, an denen sich die Ware befindet (A bis D bzw. 0 bis 3 und im LKW (Position Nr.4))\n",
    "\n",
    "$$ 6 \\cdot 6 \\cdot 4 \\cdot 5 = 720 \\texttt{ mögliche Zustände}$$\n",
    "\n",
    "Die Aktionen ($Actions$), die der LKW ausführen kann sind:\n",
    "0. nach Norden fahren\n",
    "1. nach Osten fahren\n",
    "2. nach Süden fahren\n",
    "3. nach Westen fahren\n",
    "4. Ware einsammeln\n",
    "5. Ware abladen\n",
    "\n",
    "Dabei kann er folgende Belohnungen (und Abzüge) ($Rewards$) erhalten:\n",
    "* Ware korrekt abliefern: +20\n",
    "* Ware falsch einsammeln/abliefern: -10\n",
    "* Pro Schritt: -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definitionen implementieren"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zunächst werden alle Libraries, die jetzt oder im Verlaufe der gesamten Entwicklung benötigt werden importiert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import random\n",
    "import time\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Im Folgenden werden die zuvor getroffenen Spezifikationen in ein für den Rechner verständliches Format überführt. Es gibt jeweils sechs Spalten und Reihen (0 bis 5) und eine Menge von Aktionen (ebenfalls 0 bis 5). Für die Ausgabe wird noch eine Überführung in eine Beschreibung vorgenommen. Außerdem werden die Koordinaten der Lager und Supermärkte festgehalten. Die Koordinaten haben dabei die Form `(Reihe, Spalte)`.\n",
    "\n",
    "Die Grünstreifen werden in einer Menge gespeichert. Ein Grünstreifen liegt immer zwischen zwei Feldern. Diese werden in einem Tupel in der Reihenfolge `(Links, Rechts)` bzw. `(Oben, Unten)` angegeben. Daraus ergibt sich für jedes Stück eines Grünstreifens ein Tupel der Form `((Reihe Zelle links, Spalte Zelle links), (Reihe Zelle rechts, Spalte Zelle rechts))` bzw. mit \"oben\" und \"unten\", falls es sich um einen horizontalen Streifen handelt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = [row for row in range(0, 6)]\n",
    "cols = [col for col in range(0, 6)]\n",
    "actions = {action for action in range(0, 6)}\n",
    "actions_description = [\"Drive north\", \"Drive east\", \"Drive south\", \"Drive west\", \"Pickup goods\", \"Dropoff goods\"]\n",
    "stations = [(0,0), (0,3), (5,0), (4,5)]\n",
    "stations_descriptions = [\"Warehouse A\", \"Warehouse B\", \"Supermarket C\", \"Supermarket D\"]\n",
    "position_goods_descriptions = stations_descriptions + [\"In the truck\"]\n",
    "walls = {\n",
    "    ((0,2), (0,3)), #vertikal\n",
    "    ((1,2), (1,3)),\n",
    "    ((3,1), (3,2)),\n",
    "    ((4,0), (4,1)),\n",
    "    ((5,0), (5,1)),\n",
    "    ((5,2), (5,3)),\n",
    "    ((1,0), (2,0)), # horizontal\n",
    "    ((1,5), (2,5)),\n",
    "    ((3,4), (4,4)),\n",
    "    ((3,5), (4,5))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Als nächstes soll für jedes Feld im Stadtplan festgehalten werden, welche Aktionen von diesem Feld ausgehend möglich sind. Zunächst wird allen Feldern alle Aktionen zugewiesen. Den am Rand liegenden Feldern wird die Aktion aberkannt, die aus der Stadt raus führen würde. Einsammel- und Ablieferaktionen sind nur an den zuvor definierten Stationen möglich, weshalb den anderen Feldern diese Aktion ebenfalls entzogen wird. Als nächstes werden Felder betrachtet, die in der direkten Nachbarschaft eines Grünstreifens liegen und dort die Aktionen entfernt, die den LKW dazu veranlassen würden, die Grünanlage zu zerstören. Daraus ergibt sich dann das gewünschte Dictionary mit dem Feld als Key und einer Menge möglicher Aktionen als Value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "possible_actions = dict()\n",
    "\n",
    "for row in rows:\n",
    "    for col in cols:\n",
    "        possible_actions[(row, col)] = copy.deepcopy(actions)\n",
    "for key in possible_actions:\n",
    "    (row, col) = key\n",
    "    # Ränder\n",
    "    if row == 0:\n",
    "        possible_actions[key].remove(0)\n",
    "    elif row == 5:\n",
    "        possible_actions[key].remove(2)\n",
    "    if col == 0:\n",
    "        possible_actions[key].remove(3)\n",
    "    elif col == 5:\n",
    "        possible_actions[key].remove(1)\n",
    "    # Einsammeln, Abliefern\n",
    "    if (row, col) not in stations:\n",
    "        possible_actions[key].remove(4)\n",
    "        possible_actions[key].remove(5)\n",
    "    # Grünstreifen\n",
    "    if ((row, col), (row, col + 1)) in walls:\n",
    "        possible_actions[key].remove(1)\n",
    "    if ((row, col - 1), (row, col)) in walls:\n",
    "        possible_actions[key].remove(3)\n",
    "    if ((row, col), (row + 1, col)) in walls:\n",
    "        possible_actions[key].remove(2)\n",
    "    if ((row - 1, col), (row, col)) in walls:\n",
    "        possible_actions[key].remove(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Die Transport-Klasse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Transportklasse speichert den aktuellen Zustand (`state`), die grafische Darstellung (`canvas`) von diesem sowie den aktuell erreichten Wert (`current_value`). Außerdem speichert sie, wie viele Schritte bereits ausgeführt wurden (`steps`). Ebenfalls ist eine Methode zur Ermittlung der nächsten Aktion, die `action_method`, zu übergeben. Dies kann eine Methode sein, die eine zufällige Aktion zurückgibt (`random_action`) oder eine Methode, die die durch Q-Learning erlernten Werte ausnutzt (`q_learning_action`). Außerdem werden alle besuchten Zustände in `visited_states` aufgelistet. Ist die Variable `stepwise` auf `True` gesetzt, wird zur besseren Sichtbarkeit der Aktionen in der Darstellung nach jeder Aktion eine kurze Pause durchgeführt. Die graphische Darstellung erfolgt nur, wenn `visualize` auch `True` ist. Die Variable `done` markiert, ob der Transport der Waren erfolgreich abgeschlossen wurde. Für eine spätere Berechnung wird die Vorgängerposition der Waren benötigt, weshalb diese hier ebenfalls abgespeichert wird."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transport():\n",
    "    def __init__(self, state, action_method, stepwise = False, visualize = False):\n",
    "        self.state = state\n",
    "        self.canvas = self.init_canvas()\n",
    "        self.action_method = action_method\n",
    "        self.current_value = 0\n",
    "        self.steps = 0\n",
    "        self.visited_states = [self.state]\n",
    "        self.state_list = []\n",
    "        self.stepwise = stepwise\n",
    "        self.visualize = visualize\n",
    "        self.started = False\n",
    "        self.done = False\n",
    "        self.position_goods_old = self.state[1]\n",
    "\n",
    "        self.canvas[3].on_mouse_down(self.handle_mouse_down)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transition-Funktion, Probability und Reward-Funktion\n",
    "\n",
    "Alle Schritte, die bis jetzt beschrieben wurden, befassen sich mit dem Problem. Um eine Lösung zu finden, wird eine Policy-Funktion $\\pi(s)$ aufgestellt, die für einen State $s$ eine Aktion $a$ zurückgibt, die die vermutlich höchste Belohnung für den Agenten bereithält. Die Policy-Funktion kennt Tripel der Form $<s,a,r>$ wobei $s$ der aktuelle State ist, $a$ die Aktion die im State $s$ ausgeführt wird und $r$ die Belohnung, die der Agent für die Aktion $a$ im State $s$ bekommen würde. Die Policy-Funktion hat viele von diesen Tripeln vorliegen und gibt die Aktion mit der höchsten Belohnung zurück. Eine optimale Policy $\\pi*$ gibt immer die Aktion an, die langfristig die höchste Belohnung ergibt.\n",
    "\n",
    "Der Prozess, also der Weg vom Start- zum Endzustand, hat dabei die Markov-Eigenschaft. Das bedeutet, dass der Folgezustand nur vom aktuellen Zustand abhängig ist und nicht auf den vorangegangenen basiert. Es gilt also:\n",
    "\n",
    "$P(S_{t+1}=s_t'|s_t,a_t,s_{t-1},a_{t-1})  =  P(S_{t+1}=s_t'|s_t,a_t) \\forall s \\in S$\n",
    "\n",
    "Das Markovsche Prinzip besagt, dass die vergangenen States oder Aktionen keinen Einfluss auf die aktuellen oder zukünftigen Entscheidungen haben. Die Wahrscheinlichkeit, die die Transition-Funktion berechnet ist unabhängig von den vorherigen States.\n",
    "\n",
    "Die $Transition-Funktion$ gibt an, mit welcher Wahrscheinlichkeit man von einem Zustand in den nächsten gelangt. Es handelt sich also um eine Funktion der Art $P_{ss'}(s'| s, a)$. Dabei ist $P$ die Wahrscheinlichkeit, dass Aktion $a$ vom Zustand $s$ zu $s'$ führt. Das Ergebnis der Funktion kann mit Hilfe einer Matrix dargestellt werden:\n",
    "\n",
    "\n",
    "$P = \\begin{bmatrix}\n",
    "P_{11} & ... & P_{1n}\\\\\n",
    "... &  &\\\\\n",
    "P_{n1} & ... & P_{nn}\n",
    "\\end{bmatrix}$ wobei $1$ bis $n$ alle möglichen Zustände bezeichnen\n",
    "\n",
    "Die Summe der Werte aus einer Reihe der Matrix muss für alle Reihen 1 ergeben.\n",
    "\n",
    "Für jeden Zustandswechsel kann eine Belohnung $r$ (\"Reward\") vergeben werden. Der Höhe der Belohnung ist durch die $Reward-Funktion$ festgelegt. In vielen MDPs wird die Belohnung mit einem Faktor $\\gamma$, dem \"Discount\" multipliziert. Dabei ist $\\gamma \\in [0,1]$ und wird mit der Anzahl der bisher erfolgten Schritte $t$ potenziert. Der Discount-Faktor wird oftmals genutzt, um zu erreichen, das früher erhaltene Belohnungen stärker gewichtet werden als spätere. Der aktuelle Wert des Zustands ergibt sich aus der Summe der erhaltenen Belohnungen mal des Discounts. Ziel ist es, den Wert zu maximieren.\n",
    "Es gilt:\n",
    "\n",
    "$$\\texttt{Wert} = \\sum_{k = 0}^t \\gamma^k\\cdot r_{k+1}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Das Ganze soll nun wieder an unserem Beispiel erläutert werden."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementierung 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Transition function gibt für eine Aktion den Zustand zurück der folgt, wenn man auf den aktuellen Zustand die gewünschte Aktion anwendet. Dies ist möglich, da man aus der Kombination Zustand $s_t$ und Aktion $a$ in diesem Beispiel eindeutig den Folgezustand $s_{t+1}$ bestimmen kann, die Wahrscheinlichkeit für diesen Folgezustand also bei 100% liegt. Falls die Aktion nicht möglich ist, wird der alte Zustand zurückgegeben. Es gilt dann $s_t = s_{t+1}$. Ein Zustand ist dabei folgendermaßen aufgebaut:\n",
    "\n",
    "`(Position LKW, Position Ware, Position Ziel)`.\n",
    "\n",
    "Wobei gilt:\n",
    "* Position LKW = (Reihe LKW, Spalte LKW)\n",
    "* Position Ware $\\in$ \\[0, 4\\] $\\rightarrow$ \\[Lager A, Lager B, Supermarkt C, Supermarkt D, im LKW\\]\n",
    "* Position Ziel $\\in$ \\[0, 3\\] $\\rightarrow$ \\[Lager A, Lager B, Supermarkt C, Supermarkt D\\]\n",
    "\n",
    "Bei der Funktion handelt es sich um eine Funktion der Klasse `Transport`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transition_function(self, action):\n",
    "    position_truck, position_goods, position_goal = self.state\n",
    "    if action not in possible_actions[position_truck]:\n",
    "        return self.state\n",
    "    \n",
    "    row, col = position_truck\n",
    "    if action == 0:\n",
    "        position_truck = (row - 1, col)\n",
    "    elif action == 1:\n",
    "        position_truck = (row, col + 1) \n",
    "    elif action == 2:\n",
    "        position_truck = (row + 1, col)\n",
    "    elif action == 3:\n",
    "        position_truck = (row, col - 1)\n",
    "    elif action == 4:\n",
    "        if position_goods == stations.index(position_truck):\n",
    "            position_goods = 4\n",
    "    elif action == 5:\n",
    "        if position_goods != 4:\n",
    "            return self.state\n",
    "        position_goods = stations.index(position_truck)\n",
    "            \n",
    "    return (position_truck, position_goods, position_goal) \n",
    "Transport.transition_function = transition_function\n",
    "del transition_function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`reward_function()` gibt die Belohnung zurück, die für die Aktion im aktuellen Zustand erzielt wird.\n",
    "Bei der Funktion handelt es sich ebenfalls um eine Funktion der Klasse `Transport`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reward_function(self, action):\n",
    "    reward = -1 # Pro Schritt\n",
    "    position_truck, position_goods, position_goal = self.state\n",
    "    \n",
    "    if action not in possible_actions[position_truck]: return reward\n",
    "    \n",
    "    if action == 4:\n",
    "        station_truck = stations.index(position_truck)\n",
    "        if position_goods != station_truck: # Ware falsch einsammeln\n",
    "            reward -= 10\n",
    "    elif action == 5:\n",
    "        station_truck = stations.index(position_truck)\n",
    "        if position_goal != station_truck and position_goods == 4: # Ware falsch abliefern  \n",
    "            reward -= 10\n",
    "        elif position_goal == station_truck and position_goods == 4: # Ware korrekt abliefern\n",
    "            reward += 20\n",
    "        else: # Abliefern ohne geladene Ware\n",
    "            reward -= 10\n",
    "            \n",
    "    return reward\n",
    "Transport.reward_function = reward_function\n",
    "del reward_function"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
